# Теория вероятностей

здесь я ничего не придумывала, а брала из разной литературы.

говорю огромное спасибо всем авторам! (они есть в разделе “изучить глубже”)

- Основные правила комбинаторики
    - правило умножения
        
        Правило умножения гласит, что если у нас есть несколько независимых этапов или выборов, то общее количество возможных исходов можно получить, умножив количество вариантов каждого этапа. Другими словами, если у нас есть m способов выполнить первый этап и n способов выполнить второй этап, то общее количество возможных исходов будет равно m умножить на n.
        
        Например, если у нас есть 2 цвета и 3 формы, общее количество комбинаций будет 2 умножить на 3, то есть 6.
        
    - правило перестановок
        
        Правило перестановок гласит, что количество возможных перестановок элементов в наборе равно факториалу количества элементов в наборе.
        
        Например, 5 студентов можно поставить в очередь в буфет 5*4*3*2*1=5! =120 различными способами
        
    - правило сочетаний
        
        Правило сочетаний гласит, что количество возможных комбинаций выбора k элементов из набора из n элементов (без учета порядка) определяется биномиальным коэффициентом C(n, k), который вычисляется по формуле C(n, k) = n! / (k! * (n - k)!)
        
        Например, у нас есть 5 карточек, и мы выбираем 3 из них. По правилу сочетаний получаем 10 возможных комбинаций выбора. Например: {1, 2, 3}, {2, 4, 5}, {1, 3, 5} и т.д.
        
    - правило размещений
        
        Правило размещений гласит, что количество возможных размещений k элементов из набора из n элементов (с учетом порядка) определяется по формуле A(n, k) = n! / (n - k)!.
        
        Например, у нас есть 3 разных буквы: A, B и C. Мы хотим разместить 2 из них в определенном порядке. С использованием правила размещений получаем, что количество возможных размещений будет равно A(3, 2) = 3! / (3 - 2)! = 3! / 1! = 3 * 2 = 6.
        
- Независимые события
    
    Независимые события в теории вероятностей - это такие события, при которых наступление одного из них не влияет на вероятность наступления другого события.
    
    $$
    P(A \cap B)=P(A) \cdot P(B)
    $$
    
- Формула сложения вероятностей
    
    $$
    P(A \cup B)=P(A)+P(B)-P(A \cap B)
    $$
    
- Условная вероятность
    
    Условной вероятностью события А при условии, что произошло событие В, называется величина 
    
    $$
    P(A \mid B)=P(A \cap B) / P(B)
    $$
    
- Формула полной вероятности
    
    Пусть H1, H2… Hn - полная система событий и все P(Hi) ≠ 0.
    
    $$
    P(A)=P\left(A \mid H_1\right) P\left(H_1\right)+P\left(A \mid H_2\right) P\left(H_2\right)+\ldots+P\left(A \mid H_n\right) P\left(H_n\right)
    $$
    
- Формула Байеса
    
    Пусть H1, H2… Hn - полная система событий и P(A) ≠ 0.
    
    $$
    P\left(H_i \mid A\right)=\frac{P\left(A \mid H_i\right) P\left(H_i\right)}{P\left(A \mid H_1\right) P\left(H_1\right)+\ldots+P\left(A \mid H_n\right) P\left(H_n\right)}
    $$
    
- Испытания Бернулли. Биномиальное распределение.
    
    **Испытанием Бернулли** называют случайный эксперимент с двумя возможными элементарными исходами ω1 и ω2.
    
    Распределение числа успехов в серии из n испытаний Бернулли называют **биномиальным распределением**. Вероятность того, что в серии из n испытаний Бернулли произойдет ровно k успехов, равна:
    
    $$
    P\left(S_n=k\right)=C_n^k p^k q^{n-k}
    $$
    
- Дискретная случайная величина и ее числовые характеристики: математическое ожидание и дисперсия
    
    Случайная величина называется **дискретной**, если множество ее возможных значений конечно или счетно.
    
    **Математическим ожиданием** дискретной случайной величины или ее средним значением называется 
    
    $$
    E(X)=\sum_{i=1}^n x_i p_i
    $$
    
    **Дисперсией** случайной величины называется
    
    $$
    D(X)=E(X-E(X))^2
    $$
    
    Для расчетов дисперсии более удобна формула
    
    $$
    D(X)=E\left(X^2\right)-[E(X)]^2
    $$
    
- Свойства математического ожидания
    1. E(C) = C, если C - константа;
    2. E(aX) = aE(X), где a  константа, а X - случайная величина;
    3. E(X + Y) = E(X) + E(Y);
    4. E(XY) = E(X) · E(Y), если X и Y независимые случайные величины
- Свойства дисперсии
    1. D(C) = 0, где C  - константа;
    2. D(aX) = a^2 * D(X), где a - константа; 
    3. D(X+Y) = D(X) + D(Y), если X и Y - независимые случайные величины
    4. D(X +Y ) = D(X)+D(Y)+2Cov(X,Y), где X и Y  произвольные случайные величины, а 
        
        Cov(X, Y ) = E((X − E(X)) · (Y − E(Y )) - ковариация двух случайных величин.
        
- Распределение Пуассона
    
    Дискретная случайная величина Х имеет распределение Пуассона, если она может принимать значения 0, 1, 2, 3, … k, …, а вероятность конкретного значения задается формулой 
    
    $$
    P(X=k)=\frac{\lambda^k}{k !} e^{-\lambda}
    $$
    
    Математическое ожидание **E(X) = λ**
    
    Дисперсия **D(X) = λ**
    
    **Утверждение**
    
    Пусть независимые случайные величины X и Y имеют распределение Пуассона с параметрами λ1 и λ2 соответсвенно. Тогда случайная величина X + Y также имеет распределение Пуассона с параметром λ = λ1 + λ2
    
- Ковариация и ее свойства
    
    **Ковариацией** двух случайных величин X и Y называется 
    
    $$
    \operatorname{Cov}(X, Y)=E(X-E(X))(Y-E(Y))
    
    $$
    
    Для вычислений удобнее формула 
    
    $$
    \operatorname{Cov}(X, Y)=E(X \cdot Y)-E(X) \cdot E(Y)
    $$
    
    **Свойства ковариации:**
    
    1. Если случайные величины X и Y независимы, то Cov(X, Y ) = 0;
    2. Пусть X1 = a1+b1*X и Y1 = a2+b2*Y , тогда Cov(X1, Y1) =
    b1b2Cov(X, Y).
- Корреляция и ее свойства
    
    **Корреляцией** двух случайных величин называется 
    
    $$
    \operatorname{Cor}(X, Y)=\frac{\operatorname{Cov}(X, Y)}{\sqrt{D(X)} \sqrt{D(Y)}}
    $$
    
    Свойства корреляции:
    
    1. Если X и Y независимы, то Cor(X, Y) = 0
    2. |Cor(X, Y) | ≤ 1
    3. Если Cor(X, Y) = 1,  то Y = a + bX, где b > 0.
        
        Если Cor(X, Y) = -1,  то Y = a + bX, где b < 0.
        
    4. Пусть X1 = a1 + b1*X и Y1 = a2 + b2*Y, тогда Cor(X1, Y1) = Cor(X, Y) если b1 * b2 > 0 и Cor(X1, Y1) = - Cor(X, Y), если b1 * b2 < 0
- Непрерывные случайные величины: плотность вероятности и функция распределения, их свойства
    
    **Плотностью вероятности** называют функцию **ρ(х)**, заданную на всех х на числовой прямой, такую что:
    
    1. ρ(x) ≥ 0 для всех х
    2. Площадь под функцией плотности равна 1, т.е.
        
        $$
        \int_{-\infty}^{+\infty} \rho(x) d x=1
        $$
        
    
    **Функцией распределения F(x)** случайной величины Х называется вероятность того, что случайная величина Х примет значения, меньшее или равное х, то есть F(x) = P(X ≤ x)
    
    Если случайная величина имеет плотность ρ(х), то ее функция распределения задается в виде 
    
    $$
    F(x)=\int_{-\infty}^x \rho(t) d t
    $$
    
    **Свойства функции распределения:**
    
    1. F(x) ≥ 0 для любого х
    2. Для x1 < x2 F(x1) ≤ F(x2)
    3. F(-inf) = 0 и F(inf) = 1
- Непрерывные случайные величины: математическое ожидание и дисперсия
    
    Математическое ожидание E(X) непрерывной случайной величины равно:
    
    $$
    E(X)=\int_{-\infty}^{+\infty} x \rho(x) d x
    $$
    
    Дисперсия D(X) непрерывной случайной величины X равна:
    
    $$
    \begin{aligned}D(X) & =E(X-E X)^2=E\left(X^2\right)-[E(X)]^2= \\& =\int_{-\infty}^{+\infty} x^2 \rho(x) d x-\left(\int_{-\infty}^{+\infty} x \rho(x) d x\right)^2 .\end{aligned}
    $$
    
- Нормальное распределение
    
    Непрерывная случайная величина Х имеет **нормальное (гауссовское) распределение** вероятностей на всей числовой прямой, если ее плотность распределения для всех х выражается формулой 
    
    σ > 0 и a - произвольные числа (**параметры нормального распределения**)
    
    $$
    \rho(x, a, \sigma)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-a)^2}{2 \sigma^2}}
    $$
    
    $$
    E(X)=a, D(X)=\sigma^2
    $$
    
    N(0,1) - *стандартное нормальное распределение* 
    
    **Плотность** стандартного нормального распределения:
    
    $$
    \rho(x)=\frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}
    $$
    
    Случайная величина X ∼ N(a,σ2) может быть выражена через стандартную нормальную случайную величину Z ∼ N(0,1):
    
    $$
    X=a+\sigma Z
    $$
    
    **Функция распределения** стандартной нормальной случайной величины:
    
    $$
    \Phi(x)=\int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{\frac{-t^2}{2}} d t .
    $$
    
    Для Ф(х) составлены подробные таблицы, позволяющие находить вероятность того, что случайная величина Z попадает в некоторый отрезок [a, b]:
    
    $$
    P(a \leq Z \leq b)=\Phi(b)-\Phi(a)
    $$
    
    **Функция распределения F(x) произвольной нормально случайной величины** X ∼ N (a, σ2) выражается через Ф(х):
    
    $$
    F(x)=\Phi\left(\frac{x-a}{\sigma}\right)
    $$
    
    **Теорема**
    
    Сумма двух независимых нормальных случайных величин X1 ∼ N (a1, σ12) и X2 ∼ N (a2, σ2) является нормальной случайной величиной:
    
    $$
    X_1+X_2 \sim N\left(a_1+a_2, \sigma_1^2+\sigma_2^2\right) .
    $$
    
- Неравенство Чебышева
    
    *Общая формулировка:*
    
    Пусть Y - неотрицательная случайная величина, причем E(Y) - существует. Тогда для любого ε > 0 выполняется неравенство:
    
    $$
    P(Y \geq \varepsilon) \leq \frac{E(\varepsilon)}{\varepsilon}
    $$
    
    Если в качестве случайной величины Y рассмотреть Y = |X - E(X)|, где *Х - произвольная случайная величина, у которой существует дисперсия*, то получим важный частный случай **неравенства Чебышева:**
    
    $$
    P(|X-E(X)| \geq \varepsilon) \leq \frac{1}{\varepsilon^2} D(X)
    $$
    
- Теорема Муавра-Лапласа
    
    Пусть Sn - число успехов в n испытаниях Бернулли (число n не случайное; оно не зависит от результатов испытаний). Пусть p - вероятность успеха в одном испытании 0 < p < 1. Тогда равномерно относительно a и b, где −∞ < a < b < +∞:
    
    $$
    P\left(a \leq \frac{S_n-n p}{\sqrt{n p(1-p)}} \leq b\right) \rightarrow \Phi(b)-\Phi(a) \text { при } n \rightarrow \infty
    $$
    
    Где Ф(х) - функция стандартного нормального распределения
    
    *Другими словами*
    
    нормированное число успехов Sn в серии из n испытаний Бернулли с ростом n начинает вести себя как стандартная нормальная случайная величина Z ∼ N(0,1), или 
    
    $$
    S_n \sim n p+Z \sqrt{n p(1-p)}
    $$
    
- Центральная предельная теорема для независимых и одинаково распределенных случайных величин
    
    Пусть X1, . . . , Xn - независимые одинаково распределенные случайные величины такие, что E(Xi2) < ∞, i = 1, . . . , n. Обозначим через a их математическое ожидание, через σ^2 - дисперсию, и пусть σ > 0. Тогда равномерно относительно u и v, где −∞ ≤ u ≤ v ≤ +∞:
    
    $$
    P\left(u \leq \frac{\sum_{i=1}^n X_i-n a}{\sigma \sqrt{n}} \leq v\right) \rightarrow \Phi(v)-\Phi(u) \text { при } n \rightarrow \infty \text {. }
    $$
    
    Другими словами, сумма независимых одинаково распределенных случайных величин с ростом числа слагаемых начинает вести себя как нормальная случайная величина N (na, nσ^2):
    
    $$
    \sum_{i=1}^n X_i \simeq n a+\sqrt{n} \sigma \cdot Z
    $$
    
- Двумерное непрерывное распределение
    
    Функция ρ(x,y) называется **плотностью распределения вероятностей на плоскости**, если выполнены два условия:
    
    $$
    1.\rho(x, y) \geq 0 для всех x, y \in \mathbb{R}^2.  
    2. \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \rho(x, y) d x d y=1
    $$
    
    Второе условие означает, что совокупный объем фигуры, заключенной между поверхностью, заданной функцией ρ(x, y), и плоскостью (x, y), равен 1.
    
    **Математическим ожиданием** (средним значением) случайного вектора (X, Y) называется вектор (E(X), E(Y)). Для характеристики изменчивости случайного вектора (X, Y) используется ковариационная матрица Σ:
    
    $$
    \left(\begin{array}{cc}D(X) & \operatorname{cov}(x, y) \\\operatorname{cov}(x, y) & D(Y)\end{array}\right)
    $$
    
    На главной диагонали матрицы Σ стоят дисперсии D(X) и D(Y), а на побочной диагонали Cov(X, Y).
    
    **Теорема**
    
    Координаты случайного вектора (X, Y), имеющего совместную плотность распределения ρ(x, y), **являются независимыми** случайными величинами тогда и только тогда, когда плотность распределения вероятностей этого вектора ρ(x, y) = ρ1(x) · ρ2(y), где ρ1(x) и ρ2(y)- частные плотности распределения случайных величин X и Y.


[[Julia&Ko knowledge base]]