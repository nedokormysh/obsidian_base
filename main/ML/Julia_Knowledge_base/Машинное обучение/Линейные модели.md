
- Здесь обозначу, что точно стоит понимать про линейные модели.
Руководство к действию - открыть табличку, пробежаться глазами, понять, что все эти тонкости помнишь, закрыть табличку.

| Что такое линейность                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Модель **линейная,** если она линейна по всем числовым признакам. (Важно сделать все  признаки числовыми и от них уже строить линейный функционал) <br>    Как вариант - one-hot кодирование (делаем из одного столбца несколько и расставляем 0 и 1)<br>    не забыть удалить один из полученных бинарных признаков, так как он легко определяется, если известны остальные! наличие «лишних» признаков ведёт к переобучению или вовсе ломает модель                                                                                                                                                                                                                                                                                                           |
| Мы не изучали полиномиальные или логарифмические модели. Они и не нужны. Для этого можно ввести признак-функцию (полином или логарифм, …) от исходного признака или их какой-либо комбинации.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Линейные модели **круты**, потому что *как минимум*:<br>    - простые<br>    - интерпретируемые<br>    - можем отбирать важные признаки (чем больше вес, тем важнее признак для итогового предсказания)<br>    <br>Линейные модели **не** **круты**, потому что *как минимум*:<br>    - узкий класс, хорошо решает простые задачи на небольших датасетах, иначе стоит заниматься много и долго feature engineering <br>    - много зависит от масштаба признаков, обязательно стоит задуматься об этом прежде чем обучать модельку                                                                                                                                                                                                                              |
| Важно следить за <br>    - корреляцией признаков между собой (чем она больше, тем хуже, можно оставить один из скоррелированных признаков)<br>    - корреляцией каждого признака с таргетом (чем она больше, тем лучше, скорее всего признак будет важным для модели)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **Оптимизация и регуляризация**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Функция потерь нужна для ответа на простой вопрос во время обучения - *как часто модель ошибается?* <br>    часто ? поменяем веса (запускает оптимизацию) А теперь? ок, еще меняем… (оптимизирует)<br>    <br>    По сути мы ставим задачу оптимизации. Есть функция потерь. Есть параметр весов. Нужно найти такие веса, при которых значение функции потерь минимально.                                                                                                                                                                                                                                                                                                                                                                                       |
| Градиентный спуск:<br>    w -= alpha * dL/dw<br>    Стохастический градиентный спуск:<br>    градиентный спуск на батче                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Решение задачи регресcии не единственно! Более того, оно может быть сколь угодно большим, что создает дополнительные вычислительные трудности. Отчасти для этого и нужна регуляризация.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Регуляризация - дополнительное ограничение на вектор весов.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| w0 не регуляризируют!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| в L2-регуляризации в формуле стохастического градиентного спуска нет слагаемых, связанных с числом объектов <br>    <br>в L1-регуляризации не критично, что функция недифференцируема, попадание в 0 маловероятно, так что можем просто доопределить                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| L1-регуляризация разреживает веса<br>    у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным 0 (линии уровня функции потерь пересекаются с линиями уровня L1- нормы)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| **Линейные модели в задаче регрессии**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| MSE - классная функция потерь, она:<br>    - коррелирует с идеей об измерении расстояния между векторами<br>    - с точки зрения статистики соответсвует гипотезе о том, что данные состоят из линейного ”сигнала” и нормально распределенного “шума”                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Естественно, можно решать задачу оптимизации напрямую, но это плохо, потому что нужно обращать матрицу, более того, если матрица обратима, то часто плохо обусловлена (корень из отношения наибольшего и наименьшего чисел матрицы)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Линейные модели в задаче классификации**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| В качестве предсказания берем знак произведения весов на фичи.<br>    Минимизируем сумму неверных ответов (сумму индикаторов, что знаки правильного и предсказанного не совпали)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| $М = y_i *<w, x_i>$ - отступ. Далее будем по-разному использовать M в лоссе.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 1. Перцептрон - считаем отступы только на неправильно классифицированных объектах. Важно понимать, что решение не единственно. F = max(0, -M)<br>2. SVM. Как раз из идеи выше, что решение не единственно докручиваем - будем не только искать разделяющую прямую, но и проводить ее на одинаковом удалении от обоих классов.<br>    F(M)= max(0, 1-M)<br>    Опорные вектора -  объекты ближайшие к плоскости правильно классифицированные объекты                                                                                                                                                                                                                                                                                                             |
| 3. Логистическая регрессия. <br>    Переходим к новой идее - хотим рассматривать задачу как предсказание вероятностей классов. Для этого предсказания будем прогонять через сигмоиду (1 / (1 + e^(-z))).<br>    Теперь осталось понять, как оптимизировать веса так, чтобы зашить логику получения вероятностей.<br>    Для этого необходимо применить метод максимального правдоподобия для распределения Бернулли и получить лог лосс, у которого нет явного решения, поэтому оптимизировать будем через стохастический градиентный спуск.<br>    Таким образом, после оптимизации весов получаем предсказание как сигмоиду от скалярного произведения весов на признаки. Далее все числа (они от 0 до 1) по порогу относим либо к классу 1, либо к классу 0. |
| **Подходы к многоклассовой классификации**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| 1. one-versus-all <br>2. all-versus-all<br>3. Для логистической регрессии можно обучить k моделей, а потом использовать softmax.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
[[Машинное обучение]]