# Решающие деревья

| **Идея:** Предсказываем значение целевой переменной с помощью применения последовательности простых решающих правил (предикатов)   <br><br>Предикат - взятие порога по значению какого-то признака                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **1.** выученная функция является кусочно-постоянной, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть;<br>**2.** дерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки;<br>**3.** дерево решений способно идеально приблизить обучающую выборку и ничего не выучить (то есть такой классификатор будет обладать низкой обобщающей способностью): для этого достаточно построить такое дерево, в каждый лист которого будет попадать только один объект. Следовательно, при обучении нам надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым, чтобы результат обладал хорошей обобщающей способностью. |
| **Жадный алгоритм построения решающего дерева**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **1.** Создаём вершину v.<br>**2.** Если выполнен критерий остановки Stop(Xm), то останавливаемся, объявляем эту вершину листом и ставим ей в соответствие ответ Ans(Xm), после чего возвращаем её.<br>**3.** Иначе: находим предикат (иногда ещё говорят сплит) Bj,t, который определит наилучшее разбиение текущего множества объектов Xm на две подвыборки Xℓ и Xr, максимизируя критерий ветвления Branch(Xm,j,t).<br>**4.** Для Xℓ и Xr рекурсивно повторим процедуру.                                                                                                                                                                                                                                                                                                                                                                            |
| **Ans(Xm)**, вычисляющая ответ для листа по попавшим в него объектам из обучающей выборки, может быть, например:<br>    - в случае задачи классификации — меткой самого частого класса или оценкой дискретного распределения вероятностей классов для объектов, попавших в этот лист;<br>    - в случае задачи регрессии — средним, медианой или другой статистикой;<br>    - простой моделью. К примеру, листы в дереве, задающем регрессию, могут быть линейными функциями или синусоидами, обученными на данных, попавших в лист. <br>    <br>    Зачастую полагают, что в каждом листе просто предсказывается константа.                                                                                                                                                                                                                           |
| Критерий остановки **Stop(Xm)** — функция, которая решает, нужно ли продолжать ветвление или пора остановиться.<br>Это может быть какое-то тривиальное правило: например, остановиться только в тот момент, когда объекты в листе получились достаточно однородными и/или их не слишком много                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Критерий ветвления **Branch(Xm,feature,value)** <br>Это функция, измеряющая, насколько хорош предлагаемый сплит. Чаще всего эта функция оценивает, насколько улучшится некоторая финальная метрика качества дерева в случае, если получившиеся два листа будут терминальными, по сравнению с ситуацией, когда сама исходная вершина является листом. Выбирается такой сплит, который даёт наиболее существенное улучшение. Есть и другие подходы.                                                                                                                                                                                                                                                                                                                                                                                                      |
| Константа с (ответ на значение в листе) должна минимизировать среднее значение функции потерь. (так мы эту константу и ищем, решая оптимизационную задачу)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Информативность (impurity)** -  минимальное значение усредненной функции потерь при подобранном c. <br>Чем она ниже, тем лучше объекты в листе можно приблизить константным значением.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Branch(X_m, j, t) = \|X_m\| * H(X_m) - \|X_l \| * H(X_l) - \|X_r\| * H(X_r)<br>    <br>    H - информативность, m вершина, l и r потенциальные “дети”<br>    Branch(X_m, j, t) неотрицательна                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| **для задачи регрессии**<br>    <br>При жадной минимизации **MSE** информативность — это оценка дисперсии таргетов для объектов, попавших в лист.  <br>    Оценка значения в каждом листе — это **среднее**, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше.<br>    <br>**MAE:** <br>    В листе надо предсказывать медиану, ведь именно **медиана** таргетов для обучающих примеров минимизирует MAE константного предсказателя                                                                                                                                                                                                                                                                                                                                                                                    |
| **для задачи классификации**<br>    <br>*если хотим предсказывать класс:*<br>    оптимальным предсказанием в листе будет н**аиболее частотный класс** k∗ (если функция потерь - индикатор ошибки, то есть мы стремимся к снижению доли неверно угаданных классов)<br>    <br>*если хотим предсказывать вероятностное распределение:*<br>    максимизируем логарифм правдоподобия распределения Бернулли (аналогично с лог регрессией)<br>    оценка вероятностей в листе c_k, минимизирующая H(Xm), должна быть равна p_k, то есть **доле попавших в лист объектов этого класса**                                                                                                                                                                                                                                                                      |
| `H(X_m) = - sum(p_k * log p_k)`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **Энтропия** измеряет непредсказуемость реализации случайной величины.<br>    Для константной св энтропия - 0.<br>    Наибольшего значения энтропия достигает для равномерно распределённой случайной величины                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| **критерий Джини:**<br>    <br>    Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, метрику Бриера (за которой стоит всего лишь идея посчитать MSE от вероятностей)<br>    ****                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, [метрику Бриера](https://en.wikipedia.org/wiki/Brier_score#:~:text=The%20Brier%20Score%20is%20a,as%20applied%20to%20predicted%20probabilities)<br>     (за которой стоит всего лишь идея посчитать MSE от вероятностей). Тогда информативность получится равной                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| `H(X_m) = - sum(p_k * log (1 - p_k)`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **Разбиение по категориальным признакам:**<br>    чтобы было удобно работать как с числовыми (оперировать терминами “превосходит какое-то значение”), стоит упорядочить значения принимаемых классов по неубыванию доли объектов класса 1.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Работа с пропущенными значениями:**<br>    - для категориальных признаков лучше ввести отдельную категорию “не известно”<br>    - для числовых - отдаем в оба поддерева и с весами складываем результаты (для регрессии получаем число, для классификации оценку вероятности)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **Методы регуляризации:**<br>    - ограничение по максимальной глубине дерева;<br>    - ограничение на минимальное количество объектов в листе;<br>    - ограничение на максимальное количество листьев в дереве;<br>    - требование, чтобы функционал качества Branch при делении текущей подвыборки на две улучшался не менее чем на s процентов.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| - можно проверять критерии прямо во время построения дерева, такой способ называется pre-pruning или early stopping; <br>- можно построить дерево жадно без ограничений, а затем провести стрижку (pruning), то есть удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. При этом качество стоит измерять на отдельной, отложенной выборке.                                                                                                                                                                                                                                                                                                                                                                                                                        |

- Здесь обозначу, что точно стоит понимать про решающие деревья.
Руководство к действию - открыть табличку, пробежаться глазами, понять, что все эти тонкости помнишь, закрыть табличку.
 


[[Машинное обучение]]