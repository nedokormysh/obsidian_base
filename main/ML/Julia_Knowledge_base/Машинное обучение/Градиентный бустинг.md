# Градиентный бустинг

- Здесь обозначу, что точно стоит понимать про градиентный бустинг.
Руководство к действию - открыть табличку, пробежаться глазами, понять, что все эти тонкости помнишь, закрыть табличку.
    
    
    | Бустинг, использующий деревья решений в качестве базовых алгоритмов, называется **градиентным бустингом над решающими деревьями**, **Gradient Boosting on Decision Trees**, **GBDT** |
    | --- |
    | Прямая аналогия с игрой в гольф - каждым новым ударом направляем мяч в лунку |
    | На k-ом шаге вычисляется разность между правильным ответом и текущим предсказанием композиции из k−1 алгоритмов, затем k-й алгоритм учится предсказывать эту разность |
    | a_k(x)=a_k−1(x)+b_k(x) - формула обновления композиции |
    | Интуиция из задачи регрессии с MSE - очередной алгоритм в бустинге обучается предсказывать **антиградиент функции потерь** по предсказанию модели (если возьмем производную от функции потерь, то получим ту самую разность). Это наблюдение позволяет обобщить подход построения бустинга на произвольную дифференцируемую функцию потерь. То есть заменим обучение на разность обучением на антиградиент функции потерь. 
    
    в общем случае на каждой итерации базовые алгоритмы должны приближать значения антиградиента функции потерь. |
    | Есть частный случай, в котором в качестве таргета для базового алгоритма выгоднее использовать именно «остатки» – это касается функции потерь **MAE**. |
    | Основные библиотеки реализаций:
    - LightGBM
    - XGBoost
    - CatBoost |
    | LightGBM строит деревья по принципу: «На каждом шаге делим вершину с наилучшим скором», а основным критерием остановки выступает максимально допустимое количество вершин в дереве. Это приводит к тому, что деревья получаются несимметричными, то есть поддеревья могут иметь разную глубину – например, левое поддерево может иметь глубину 2
    , а правое может разрастись до глубины 15. С одной стороны, это позволяет быстро подогнаться под обучающие данные. С другой, бесконтрольный рост дерева в глубину неизбежно ведет к переобучению, поэтому LightGBM позволяет помимо количества вершин ограничивать и максимальную глубину. Впрочем, это ограничение обычно все равно выше, чем для XGBoost и CatBoost. |
    | XGBoost строит деревья по принципу: «Строим дерево последовательно по уровням до достижения максимальной глубины». Отдельного ограничения на количество вершин нет, так как оно естественным образом получается из ограничения на глубину дерева. В XGBoost деревья «стремятся» быть симметричными по глубине, и в идеале получается полное бинарное дерево, если это не противоречит другим ограничениям (например, ограничению на минимальное количество объектов в листе). Такие деревья обычно являются более устойчивыми к переобучению. |
    | CatBoost строит деревья по принципу: «Все вершины одного уровня имеют одинаковый предикат». Одинаковые сплиты во всех вершинах одного уровня позволяют избавиться от ветвлений (конструкций if-else) в коде инференса модели с помощью битовых операций и получить более эффективный код, который в разы ускоряет применение модели, в особенности в случае применения на батчах. Кроме этого, такое ограничение на форму дерева выступает в качестве сильной регуляризации, что делает модель более устойчивой к переобучению. Основным критерием остановки, как и в случае XGBoost, является ограничение на глубину дерева. Однако, в отличие от XGBoost, в CatBoost всегда создаются полные бинарные деревья, несмотря на то, что в некоторые поддеревья может не попасть ни одного объекта из обучающей выборки. |