# Метрики

- Здесь обозначу, что точно стоит понимать про метрики.
Руководство к действию - открыть табличку, пробежаться глазами, понять, что все эти тонкости помнишь, закрыть табличку.
    
    
    | Функция потерь ≠ метрика качества! |
    | --- |
    | **Классификация** |
    | **accuracy** - доля объектов, для которых мы правильно предсказали класс |
    | **error_rate**=1−accuracy |
    | недостатки: 
    - не учитывает дисбаланс классов
    - не учитывает цену ошибки на объектах разных классов |
    | **Confusion matrix (матрица ошибок)**
    
    класс, который представляет для нас интерес, называется «положительным», а оставшийся – «отрицательным»
    
    возможно 4 ситуации: 
    - мы предсказали положительную метку и угадали. Будет относить такие объекты к true positive (**TP**) группе (true – потому что предсказали мы правильно, а positive – потому что предсказали положительную метку);
    - мы предсказали положительную метку, но ошиблись в своём предсказании – false positive (**FP**) (false, потому что предсказание было неправильным);
    - мы предсказали отрицательную метку и угадали – true negative (**TN**);
    -  мы предсказали отрицательную метку, но ошиблись – false negative (**FN**). 
    
    **первая часть названия группы показывает угадали ли мы с классом, а вторая – какой класс мы предсказали** |
    | Confusion matrix очень показательна, но на ней нет соотношения порядка, поэтому с ее помощью трудно сравнивать модели между собой |
    | **Precision** - доля правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом
    **Precision = TP / (TP + FP)** |
    | **Recall** - доля правильно найденных положительных объектов среди всех объектов положительного класса
    **Recall = TP / (TP + FN)** |
    | **F1-мера** - среднее гармоническое от precision и recall
    F1-мера предполагает одинаковую важность Precision и Recall, можно задать коэффициент бетта и воспользоваться F_бетта. |
    | **Бинарная классификация: вероятности классов** |
    | Многие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу.
    При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. |
    | **TPR**(**true positive rate**) – это полнота, доля положительных объектов, правильно предсказанных положительными
    **TPR = TP / (TP + FN)** |
    | **FPR** (**false positive rate**) – это доля отрицательных объектов, неправильно предсказанных положительными
    **FPR = FP / (FP + FN)** |
    | Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется **ROC-кривой** (**receiver operating characteristics curve**, сокращённо **ROC curve**)
    
    Чем лучше классификатор разделяет два класса, тем больше площадь (*area under curve)* под ROC-кривой |
    | **AUC** равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, т.е. предсказание классификатора на первом объекте больше
    
    любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC |
    | **Average Precision**
    
    Будем постепенно уменьшать порог бинаризации. При этом полнота будет расти от 0 до 1, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет). Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую.
    Рассмотрим среднее значение точности (оно равно площади под кривой точность-полнота) и получим показатель эффективности, который называется **average precision** |
    | **Многоклассовая классификация**  |
    | Если классов становится больше двух, расчёт метрик усложняется. Если задача классификации на K классов ставится как K задач об отделении класса i от остальных (i=1,…,K), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из K матриц ошибок:
    
    **1.** Усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами. Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют **микроусреднением.**
    **2.** Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют **макроусреднением.** |
    | **Регрессия** |
    | MSE
    RMSE - корень из MSE 
    Rˆ2 - показывает, какая доля дисперсии таргетов объяснена моделью 
    MAE |
    | Метрики, учитывающие относительные ошибки
    MAPE
    SMAPE 
    WAPE 
    RMSLE |