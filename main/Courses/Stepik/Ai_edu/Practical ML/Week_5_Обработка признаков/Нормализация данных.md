Масштабы, в которых измеряются числовые величины в данных, могут быть разными.

- При обучении некоторых моделей разные масштабы данных могут привести к некорректным результатам (например, при обучении KNearestNeighbors или же линейных моделей с регуляризацией).
- При обучении практически всех моделей (кроме деревьев и их композиций) разные масштабы данных усложнят процесс минимизации функции потерь, так как градиентный спуск с бОльшим трудом будет вычислять градиенты по признакам разного масштаба и потому обновлять веса с разной скоростью, что приведет к медленной сходимости.
- При интерпретации линейных моделей мы можем сравнивать между собой веса только в случае, если признаки имеют одинаковый масштаб.

Поэтому и для обучения, и для интерпретации моделей нужно масштабировать данные!

Исключение составляют деревья и их композиции.

**MinMaxScaler и StandardScaler**

- Первый способ масштабировать данные называется MinMaxScaler. При применении этого метода по каждому столбцу вычисляются 𝑚𝑖𝑛min и 𝑚𝑎𝑥max, а затем значения столбца преобразуются по формуле

$𝑥→\frac{𝑥−𝑚𝑖𝑛}{𝑚𝑎𝑥−𝑚𝑖𝑛}$


При таком способе масштабирования все значения признака переходят в отрезок [0;1][0;1].

- Второй способ масштабирования - StandardScaler. При применении этого метода по каждому столбцу вычисляются среднее значение 𝑚𝑒𝑎𝑛 и стандартное отклонение 𝑠𝑡𝑑, а затем значения столбца преобразуются по формуле

$𝑥→\frac{𝑥−𝑚𝑒𝑎𝑛}{𝑠𝑡𝑑}$

При этом способе масштабирования все значения признака переходят в окрестность 0 (то есть среднее значение становится равным нулю, а остальные значения находятся вокруг него). 

В случае, если в данных нет выбросов, то большинство значений признака попадет в отрезок [−3;3][−3;3], но гарантий на это нет. В любом случае после применения такого масштабирования у нас есть уверенность в том, что признак теперь измеряется в единицах! 

