
Это еще один метод кластеризации, основанный на графовом подходе. Представим нашу выборку в виде графа, как мы уже раньше делали. Вершины графа - это объекты, а на ребрах графа можно ставить веса по-разному:

- вес на ребре 𝑤𝑖𝑗 между объектами 𝑖 и 𝑗 может быть равен или пропорционален расстоянию между объектами
- а можно построить k-nn граф, то есть каждый объект соединять только с k ближайшими соседями. Тогда 𝑤𝑖𝑗​ - это тоже или расстояние, или просто 1 - если соединение (ребро) есть, и 0 - если нет.

Итак, пусть у нас есть граф с весами на ребрах, который представляет наши данные. Введем несколько обозначений:

- 𝑊 - матрица смежности графа, то есть матрица, где на месте 𝑖,𝑗 стоит вес 𝑤𝑖𝑗, или 0 - если ребра нет
- степень вершины 𝑖 - это число 𝑑𝑖=∑𝑗𝑤𝑖𝑗​ - сумма весов ребер, соединенных с вершиной 𝑖
- 𝐷=𝑑𝑖𝑎𝑔(𝑑1,𝑑2,...,𝑑𝑙) - диагональная матрица

Тогда𝐿=𝐷−𝑊.
L - очень важная матрица, она называется _лапласианом графа_.

**Гипотеза:** Пусть есть похожие объекты 𝑥𝑗​ и 𝑥𝑘​, то есть расстояние между ними невелико. Тогда для собственных векторов 𝑓𝑖​, соответствующих маленьким собственным значениям матрицы 𝐿, выполнено 𝑓𝑖𝑗≈𝑓𝑖𝑘​.


Итак, сформулируем алгоритм спектральной кластеризации:

1. Строим по объектам граф и вычисляем его лапласиан 𝐿 = 𝐷 − 𝑊
2. Находим нормированные собственные векторы 𝑢1,... ,𝑢𝑚  матрицы 𝐿, соответствующие 𝑚 наименьшим собственным значениям
3. Составляем матрицу 𝑈 =(𝑢1∣ ... ∣𝑢𝑚), то есть матрицу, столбцами которой являются найденные собственные векторы
4. Обучаем на этой матрице алгоритм K-Means c 𝑘 кластерами.

В матрице 𝑈 столько строк, сколько объектов, и 𝑚 столбцов - поэтому ее можно воспринимать как матрицу объект-признак. Новыми признаками будут координаты собственных векторов 𝑢1,...,𝑢𝑚​. Если гипотеза с предыдущего шага верна, то для похожих объектов соответствующие координаты векторов 𝑢1,...,𝑢𝑚​ будут близки, поэтому для поиска похожих объектов можно обучить на матрице 𝑈 алгоритм K-Means!


**Плюсы и минусы спектральной кластеризации**

Плюсы:

- Применима для данных высокой размерности (с большим числом признаков). Это возможно благодаря графовой структуре алгоритма
- Нет ограничений на форму кластеров. Значит, метод может находить кластеры сложных форм

Минусы:

- Относительно медленный алгоритм - он точно медленнее K-Means
- Менее изученный и поэтому гораздо реже применяемый, чем K-Means и иерархическая кластеризация 
- Довольно сложный для объяснения
- Зависит от случайности (так как его часть - это K-Means, зависящий от выбора изначальных центров кластеров)
- Требует выбора числа кластеров

[[Кластеризация данных]]