
При снижении размерности мы не выкидываем признаки, а придумываем новые признаки на основе исходных - так, чтобы новых признаков было меньше, чем старых.

_метод главных компонент_ - это метод снижения размерности, в котором новые признаки являются линейными комбинациями исходных.

# **Метод главных компонент**

Постановка задачи:

- имеются 𝑥1,...,𝑥𝑛 - исходные числовые признаки
- хотим найти 𝑧1,...,𝑧𝑑​ – новые числовые признаки, 𝑑≤𝑛

У нас есть два требования:

1. чтобы новые числовые признаки 𝑧 _линейно_ выражались через исходные признаки 𝑥
2. чтобы при переходе к новым признакам _было потеряно наименьшее количество исходной информации_

**Требование линейности**

Мы хотим, чтобы новые признаки линейно выражались через старые, то есть каждый новый признак

$z_j​=u_{j1}​x_1​+u_{j2}​x_2​+...+u_{jn}​x_n$
линейно выражается через исходные.

Существует наглядная геометрическая интерпретация этого требования: оно означает, что каждый новый признака 𝑧𝑗​ - это проекция исходного пространства признаков на некоторый вектор 𝑢𝑗​ (компоненту). Тогда в совокупности метод главных компонент - это проекция 𝑛-мерного пространства признаков на некоторое 𝑑-мерное подпространство (заданное векторами 𝑢𝑗​).

**Требование о потере наименьшего количества информации**

_В методе главных компонент мы хотим, чтобы при переходе к новым признакам было потеряно наименьшее количество исходной информации._

**Рассмотрим пример:** пусть у нас есть точки в двумерном пространстве (у каждого объекта две координаты), и мы хотим их спроецировать на некоторую прямую (одномерное подпространство), потеряв наименьшее количество информации об исходных данных.

- _Дисперсия (разброс) выборки, посчитанная в новых признаках, показывает, как много информации нам удалось сохранить после понижения размерности, поэтому дисперсия в новых признаках должна быть максимальной_

## **Математическая постановка задачи метода главных компонент**


Для снижения размерности нам нужно найти векторы 𝑢𝑖​, на которые проецируются исходные данные. Эти векторы называются _компонентами_.

Будем искать такие компоненты 𝑢1,𝑢2,...,𝑢𝑑​, что:

1) они ортогональны, то есть (𝑢𝑖,𝑢𝑗)=0
2) они нормированы, то есть ∣∣𝑢𝑖∣∣=1
3) дисперсия проекции выборки на них максимальна (требование сохранения максимума информации о данных):

𝐷(𝑋𝑢𝑖)→𝑚𝑎𝑥𝑢𝑖,𝑖=1,...,𝑑


**Дисперсия проекции**

Для того, чтобы дальнейшие преобразования были математически верны, необходимо _центрировать данные._ 

_Центрировать данные -_ это вычесть из каждого признака его среднее значение, чтобы новое среднее по каждому признаку было равно 0.

Теперь запишем дисперсию проекции в более удобном для нас виде:

- Проекция выборки 𝑋 на вектор (компоненту) 𝑢𝑖​ - это вектор 𝑋𝑢𝑖
- Тогда требование на максимизацию дисперсию проекции выборки 𝑋 на подпространство 𝑈𝑑={𝑢1,...,𝑢𝑑} можно записать как 

$∑_{𝑖=1}^𝑑∣∣𝑋𝑢𝑖∣∣^2→𝑚𝑎𝑥_𝑢$

(это неочевидный факт, но он выводится из определения дисперсии и из того, что данные центрированы)

u1​ - собственный вектор матрицы (ковариации) $𝑋^𝑇𝑋$ с максимальным собственным значением.

- Мы выяснили, что 𝑢1​ - собственный вектор матрицы (ковариация) $𝑋^𝑇𝑋$ с максимальным собственным значением.
- На следующем шаге, рассуждая аналогично, находим вектор 𝑢2​ - собственный вектор матрицы $𝑋^𝑇𝑋$ со следующим по величине собственным значением.
- И так далее. Если мы хотим снизить размерность пространства, скажем, до пяти, то нам нужно найти первые пять собственных векторов матрицы $𝑋^𝑇𝑋$ с пятью наибольшими собственными значениями.

**Сколько компонент нам нужно?**

- Упорядочим собственные значения матрицы 𝑋𝑇𝑋по убыванию:

$𝜆1≥𝜆2≥...≥𝜆𝑛≥0$

- Оказывается, что доля дисперсии, объясненной первыми 𝑘 компонентами_ - это

$𝛿𝑘=\frac{𝜆1+...+𝜆𝑘}{𝜆1+...+𝜆𝑛}$​​

- Тогда доля необъясненной дисперсии - это

$𝐸𝑘=1−\frac{𝜆1+...+𝜆𝑘}{𝜆1+...+𝜆𝑛}$

На практике обычно бывает так, что при некотором числе компонент 𝑚 доля необъясненной дисперсии резко падает, а затем уже практически не убывает. Тогда при снижении размерности при помощи метода главных компонент берут 𝑚 компонент. Этот метод называется _критерием крутого склона_.

[[Снижение размерности данных]]