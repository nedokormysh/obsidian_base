
Если при помощи градиентного спуска мы ищем минимум функции многих переменных, то алгоритм метода ровно такой же, как и в одномерном случае:

- Выбираем некоторую произвольную точку старта 𝑤=(𝑤0,𝑤1,...,𝑤𝑑)
- На каждой следующей итерации вектор параметров обновляется по формуле 𝑤𝑛𝑒𝑤=𝑤𝑜𝑙𝑑−𝜂∇𝑄(𝑤𝑜𝑙𝑑)

Это векторная запись. Её можно расписать для каждого веса в отдельности:

$$𝑤_0^{𝑛𝑒𝑤}=𝑤_0^{𝑜𝑙𝑑}−𝜂\frac{∂𝑄}{∂𝑤_0}(𝑤^{𝑜𝑙𝑑})$$

$$𝑤_1^{𝑛𝑒𝑤}=𝑤_1^{𝑜𝑙𝑑}−𝜂\frac{∂𝑄}{∂𝑤1}(𝑤^{𝑜𝑙𝑑})$$
и т. д.

**Векторная запись формул**

Из предыдущих шагов урока хорошо видно, что обновлять каждый вес по-отдельности и сложно,  и неудобно (что вручную, что в коде). Поэтому когда мы имеем дело с функциями многих переменных, гораздо удобнее работать с ними в векторном виде.

Например, вектор предсказаний линейной регрессии на матрице объект-признак X имеет вид

𝑎(𝑋)=𝑋𝑤,

 то есть для получения предсказаний матрица объект-признак 𝑋X умножается на вектор весов 𝑤.

Ошибку модели также можно переписать в векторном виде:

$$𝑀𝑆𝐸(𝑤)=\frac{1}{𝑙}∣∣𝑋𝑤−𝑦∣∣^2$$

где 𝑦 - вектор правильных ответов,  𝑙 - число объектов в выборке,  а ∣∣𝑋𝑤−𝑦∣∣ - длина (норма) вектора 𝑋𝑤−𝑦.

