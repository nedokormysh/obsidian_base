
**Вычислительные затраты градиентного спуска**

Градиентный спуск - численный метод, позволяющий искать минимумы различных,  даже очень сложных функций потерь. Однако для его реализации требуются большие вычислительные ресурсы.

Напомним, что на каждой итерации метода мы вычисляем градиент функции потерь:

$$∇𝑄(𝑤)=∑_{𝑖=1}^𝑙∇𝑞_𝑖(𝑤)$$
который равен сумме градиентов по всем объектам выборки.

Поэтому на каждой итерации градиентного спуска необходимо

- сделать много вычислений,  а именно,  вычислить значения 𝑙×𝑑 частных производных (где 𝑙 - число объектов, 𝑑 - число признаков)
- хранить эти вычисления в памяти

Обычно для сходимости требуются сотни или тысячи итераций.

Поэтому при больших размерах выборки и/или большом числе признаков градиентный спуск будет работать очень долго.

**Стохастический градиентный спуск (Stochastic Gradient Descent, SGD)**

Для ускорения работы метода было принято на каждой итерации выбирать один случайный объект выборки и вычислять градиент только по нему, а не по всей выборке. Этот метод называется _стохастическим градиентным спуском_.

Алгоритм:

- Выбираем случайным образом стартовые параметры метода 𝑤0w0​
- На каждой итерации:
    - выбираем случайным образом один объект из выборки (пусть 𝑖𝑛𝑑 - его индекс)
    - обновляем веса по формуле 𝑤𝑛𝑒𝑤=𝑤𝑜𝑙𝑑−𝜂∇𝑞𝑖𝑛𝑑(𝑤𝑜𝑙𝑑),    
        где ∇𝑞𝑖𝑛𝑑(𝑤𝑜𝑙𝑑) - градиент функции потерь, посчитанный на объекте с индексом 𝑖𝑛𝑑ind и вычисленный в точке 𝑤𝑜𝑙𝑑wold​.

_Доказано, что если классический градиентный спуск может найти минимум функции, то с этой задачей справится и стохастический градиентный спуск (для этого методу SGD потребуется чуть больше итераций)._

Стохастический градиентный спуск менее точно вычисляет градиент функции потерь на каждой итерации, но в результате большого числа итераций и из-за случайного выбора объектов неточности усредняются, и метод верно находит минимум.

**Mini-Batch Gradient Descent**

Если классический градиентный спуск работает долго, но он очень точно указывает направление уменьшения функции на каждой итерации, то стохастический градиентный спуск, наоборот,  очень быстрый, но на каждой итерации довольно неточно вычисляет градиент (так как использует для этого только один объект выборки).

На практике хотелось бы соблюсти баланс скорости работы и точности вычислений, поэтому чаще всего используют _mini-batch gradient descent (градиентный спуск по мини-батчам)._ Метод заключается в том, что на каждой итерации:

- выбираем группу случайных объектов (_batch_) заранее заданного размера,  например, 𝑁=64 объекта.   
    Пусть 𝑖1,𝑖2,...,𝑖𝑁​ - их индексы
- градиент функции потерь считаем как среднее значение градиентов по объектам батча:

$$𝑤_{𝑛𝑒𝑤}=𝑤_{𝑜𝑙𝑑}−\frac{𝜂}{𝑁}∑_{𝑖=1}^𝑁∇𝑞_𝑖(𝑤_{𝑜𝑙𝑑})$$

При таком подходе и вычислений мы делаем гораздо меньше, чем в классическом градиентном спуске (_Batch GD_), и при этом довольно точно вычисляем градиент.

**Другие модификации градиентного спуска**

Вы узнали,  как модифицировать градиентный спуск,  чтобы он работал и быстро, и точно. Однако у метода есть ещё некоторые особенности. Например,

- метод может застрять в локальном минимуме и не найти глобальный минимум (а мы ищем именно его, так как мы ищем точку, в которой функция потерь принимает наименьшее значение)
- метод может медленно обучаться по каким-то весам, а по каким-то очень быстро - и этот дисбаланс препятствует поиску оптимального решения