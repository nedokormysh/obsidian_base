
**Бинарный классификатор**

Давайте посмотрим на ответ. Формула для _бинарного линейного классификатора_:

𝑎(𝑥)=𝑠𝑖𝑔𝑛(𝑤0+𝑤1𝑥1+...+𝑤𝑑𝑥𝑑)=𝑠𝑖𝑔𝑛((𝑤,𝑥)),

то есть знак (_sign_) от вычисленной суммы (или же знак скалярного произведения (𝑤,𝑥)(w,x)).

- если (𝑤,𝑥)>0, то 𝑎(𝑥)=𝑠𝑖𝑔𝑛((𝑤,𝑥))=+1, то есть объект принадлежит положительному классу
- если (𝑤,𝑥)<0, то 𝑎(𝑥)=𝑠𝑖𝑔𝑛((𝑤,𝑥))=−1, то есть объект принадлежит отрицательному классу

**Линейный классификатор**

Классификаторы бывают линейными и нелинейными.

_Линейный классификатор_ - это классификатор, разделяющий классы линейной поверхностью (более точно - гиперплоскостью).  
Если же разделяющая поверхность нелинейная, то это уже нелинейный классификатор.

Оказывается, что классификатор, делающий предсказания по формуле 𝑎(𝑥)=𝑠𝑖𝑔𝑛(𝑤,𝑥), линейный.

Почему так? Вы уже знаете, что если 𝑠𝑖𝑔𝑛(𝑤,𝑥)>0 то объект относится к классу +1+1, а если 𝑠𝑖𝑔𝑛(𝑤,𝑥)<0 то к классу −1.   
То есть разделяющая граница между классами задается уравнением

(𝑤,𝑥)=𝑤0+𝑤1𝑥1+...+𝑤𝑑𝑥𝑑=0,

а это гиперплоскость, то есть линейная поверхность.

В частности, если у объекта два признака 𝑥1​ и 𝑥2​, то уравнение разделяющей границы 𝑤0+𝑤1𝑥1+𝑤2𝑥2=0 - это уравнение прямой линии.

**Красивая форма записи**

Если классы обозначены не как +1 и −1, а как 1 и 0, то иногда используют другую форму записи линейного классификатора:

𝑎(𝑥)=[(𝑤,𝑥)>0].

Здесь [(𝑤,𝑥)>0][(w,x)>0] - индикатор события (𝑤,𝑥)>0(w,x)>0.  
Он равен 11, если событие выполняется, т.е. если (𝑤,𝑥)>0, и 0иначе.

_Комментарий: модель называется линейной, если она линейна по весам, то есть веса входят в формулу в первых степенях. Признаки могут входить в формулу в любом виде (в первой степени, в квадрате, могут быть перемножены и так далее) - от этого модель не перестает быть линейной._

Почти каждой модели машинного обучения соответствуют **две** функции:

- формула, по которой модель делает предсказания
- функция потерь, которая минимизируется при обучении модели.

Для линейной регрессии, напомню, эти функции следующие:

- 𝑎(𝑥)=(𝑤,𝑥) - формула для предсказаний
- 𝑀𝑆𝐸 =1𝑙∑𝑖=1𝑙(𝑎(𝑥𝑖)−𝑦𝑖)2 - функция потерь.

Мы уже знаем, что линейный классификатор делает предсказания по формуле 𝑎(𝑥)=𝑠𝑖𝑔𝑛(𝑤,𝑥).  
Осталось разобраться, как он обучается.  
Другими словами, требуется понять, какую функцию потерь нужно минимизировать, чтобы найти оптимальные веса.

Самый разумный способ считать ошибку в задаче классификации - это посчитать долю ошибочных предсказаний модели, то есть

1𝑙∑𝑖=1𝑙[𝑎(𝑥𝑖)≠𝑦𝑖],

где [𝑎(𝑥𝑖)≠𝑦𝑖]=1, если предсказанный класс не совпал с правильным, и 0 иначе.

**Аппроксимация эмпирического риска**

Так как пороговая функция потерь 𝐿(𝑤)- не лучший выбор для обучения классификатора, то вместо неё обычно берут другую функцию потерь 𝐿~(𝑤), которая

- ограничивает сверху пороговую функцию потерь: 𝐿(𝑤)≤𝐿~(𝑤)
- является непрерывной (а лучше гладкой)

Тогда при решении задачи минимизации (то есть при обучении модели) функции потерь 𝐿~(𝑤)
𝐿(𝑤)≤𝐿~(𝑤)→𝑚𝑖𝑛𝑤

автоматически минимизируется исходная, пороговая функция потерь 𝐿(𝑤).




Мы поняли, что для решения задачи классификации нам нужно выбрать некоторую гладкую (или хотя бы непрерывную) функцию потерь 𝐿~(𝑤), ограничивающую сверху пороговую функцию потерь. Таких функций 𝐿~(𝑤) существует очень много, и _**от выбора конкретной функции будет зависеть то, какие в итоге получатся веса у модели после минимизации. Собственно, сама модель определяется тем, какая функция потерь будет выбрана!**_

Существует несколько широко используемых функций потерь 𝐿~(𝑤) в задаче классификации. Каждая из них задает свой классификатор, со своим названием и особенностями. Подробно про основные классификаторы и их особенности мы поговорим в следующих уроках.