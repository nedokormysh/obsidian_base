**Логистическая регрессия**

Один из самых известных в машинном обучении линейных классификаторов - это логистическая регрессия. С неё и начнем говорить о различных моделях классификации.

Когда мы решаем задачу классификации, мы можем захотеть предсказывать не только классы, но и их вероятности. Для целей бизнеса, например, полезно знать, с какой вероятностью клиент вернет кредит (а не просто - вернет кредит или не вернет). Например, если клиент вернет кредит с вероятностью 0.95, то это очень надежный клиент. А если с вероятностью 0.52  - этот клиент под вопросом. Но с точки зрения бинарной классификации обоим клиентам модель выдаст кредит, так как стандартный порог перевода вероятности в классы - 0.5. 

- задача классификации, в которой модель предсказывает только классы - это **жесткая классификация**
- если же классификатор предсказывает вероятности классов - это **мягкая классификация**


**Формула для предсказания логистической регрессии**

Формула для предсказания классов у линейного классификатора, как вы помните, такая

𝑎(𝑥)=𝑠𝑖𝑔𝑛(𝑤,𝑥).

Нам нужно видоизменить эту формулу, чтобы на выходе получать вероятность.

Вероятность - это число на отрезке [0;1][0;1].

В логистической регрессии для предсказания вероятности используется **сигмоида:**

**𝑎(𝑥)=𝜎(𝑤,𝑥),**

где $𝜎(𝑧)=\frac{1}{1+𝑒^{−𝑧}}$

То есть формула для предсказания логистической регрессии имеет вид

$$𝑎(𝑥)=𝜎(𝑤,𝑥)=\frac{1}{1+𝑒^{−(𝑤,𝑥)}}$$


По оси абсцисс (горизонтальная ось) на графике отложено значение скалярного произведения (𝑤,𝑥), а по оси ординат - предсказанная моделью вероятность того, что объект 𝑥x принадлежит к классу +1

**Логистическая регрессия - это линейный классификатор**

Давайте разберемся, является ли логистическая регрессия _линейным_ классификатором?

Для этого поймем, как задается разделяющая поверхность между классами в логистической регрессии.

По умолчанию порог перевода вероятности в класс - 0.5. То есть если вероятность принадлежности объекта к положительному классу больше  0.5, то модель относит объект к классу +1. А если меньше 0.5 - к классу −1.

Если записать это в терминах формул, получим:

- 𝑎(𝑥)=𝜎(𝑤,𝑥)>0.5⇒класс+1
- 𝑎(𝑥)=𝜎(𝑤,𝑥)< 0.5⇒класс−1.

Получаем, что разделяющая граница задается уравнением 𝜎(𝑤,𝑥)=0.5. Посмотрим на график. Значение 𝑦=0.5 соответствует точке (𝑤,𝑥)=0 на оси абсцисс.

Таким образом, логистическая регрессия является **линейным классификатором.**

**Функция потерь в логистической регрессии? Это не MSE!**

Мы поговорили о том, по какой формуле логистическая регрессия делает предсказания.  
Осталось понять, **_какая функция потерь используется при обучении модели_**?

Так как логистическая регрессия предсказывает вероятность, то есть непрерывную величину, то можно попробовать взять функцию потерь регрессии, например, среднеквадратичную ошибку (MSE). Тогда задача оптимизации для логистической регрессии будет выглядеть так:

𝑀𝑆𝐸=1𝑙∑𝑖=1𝑙(𝑎(𝑥𝑖)−𝑦𝑖)2=1𝑙∑𝑖=1𝑙(11+𝑒−(𝑤,𝑥𝑖)−𝑦𝑖)2→min⁡𝑤MSE=l1​i=1∑l​(a(xi​)−yi​)2=l1​i=1∑l​(1+e−(w,xi​)1​−yi​)2→wmin​

Если использовать MSE как функцию потерь в этой задаче, то столкнемся с двумя трудностями:

- полученная функция _невыпуклая_ (то есть может иметь несколько локальных минимумов), поэтому при поиске минимума методом градиентного спуска мы можем не попасть в глобальный минимум,  а застрять в локальном