**Логистическая регрессия**

Один из самых известных в машинном обучении линейных классификаторов - это логистическая регрессия. С неё и начнем говорить о различных моделях классификации.

Когда мы решаем задачу классификации, мы можем захотеть предсказывать не только классы, но и их вероятности. Для целей бизнеса, например, полезно знать, с какой вероятностью клиент вернет кредит (а не просто - вернет кредит или не вернет). Например, если клиент вернет кредит с вероятностью 0.95, то это очень надежный клиент. А если с вероятностью 0.52  - этот клиент под вопросом. Но с точки зрения бинарной классификации обоим клиентам модель выдаст кредит, так как стандартный порог перевода вероятности в классы - 0.5. 

- задача классификации, в которой модель предсказывает только классы - это **жесткая классификация**
- если же классификатор предсказывает вероятности классов - это **мягкая классификация**


**Формула для предсказания логистической регрессии**

Формула для предсказания классов у линейного классификатора, как вы помните, такая

𝑎(𝑥)=𝑠𝑖𝑔𝑛(𝑤,𝑥).

Нам нужно видоизменить эту формулу, чтобы на выходе получать вероятность.

Вероятность - это число на отрезке [0;1][0;1].

В логистической регрессии для предсказания вероятности используется **сигмоида:**

**𝑎(𝑥)=𝜎(𝑤,𝑥),**

где $𝜎(𝑧)=\frac{1}{1+𝑒^{−𝑧}}$

То есть формула для предсказания логистической регрессии имеет вид

$$𝑎(𝑥)=𝜎(𝑤,𝑥)=\frac{1}{1+𝑒^{−(𝑤,𝑥)}}$$


По оси абсцисс (горизонтальная ось) на графике отложено значение скалярного произведения (𝑤,𝑥), а по оси ординат - предсказанная моделью вероятность того, что объект 𝑥x принадлежит к классу +1

**Логистическая регрессия - это линейный классификатор**

Давайте разберемся, является ли логистическая регрессия _линейным_ классификатором?

Для этого поймем, как задается разделяющая поверхность между классами в логистической регрессии.

По умолчанию порог перевода вероятности в класс - 0.5. То есть если вероятность принадлежности объекта к положительному классу больше  0.5, то модель относит объект к классу +1. А если меньше 0.5 - к классу −1.

Если записать это в терминах формул, получим:

- 𝑎(𝑥)=𝜎(𝑤,𝑥)>0.5⇒класс+1
- 𝑎(𝑥)=𝜎(𝑤,𝑥)< 0.5⇒класс−1.

Получаем, что разделяющая граница задается уравнением 𝜎(𝑤,𝑥)=0.5. Посмотрим на график. Значение 𝑦=0.5 соответствует точке (𝑤,𝑥)=0 на оси абсцисс.

Таким образом, логистическая регрессия является **линейным классификатором.**

**Функция потерь в логистической регрессии? Это не MSE!**

Мы поговорили о том, по какой формуле логистическая регрессия делает предсказания.  
Осталось понять, **_какая функция потерь используется при обучении модели_**?

Так как логистическая регрессия предсказывает вероятность, то есть непрерывную величину, то можно попробовать взять функцию потерь регрессии, например, среднеквадратичную ошибку (MSE). Тогда задача оптимизации для логистической регрессии будет выглядеть так:

𝑀𝑆𝐸=1𝑙∑𝑖=1𝑙(𝑎(𝑥𝑖)−𝑦𝑖)2=1𝑙∑𝑖=1𝑙(11+𝑒−(𝑤,𝑥𝑖)−𝑦𝑖)2→min⁡𝑤

Если использовать MSE как функцию потерь в этой задаче, то столкнемся с двумя трудностями:

- полученная функция _невыпуклая_ (то есть может иметь несколько локальных минимумов), поэтому при поиске минимума методом градиентного спуска мы можем не попасть в глобальный минимум,  а застрять в локальном
- - MSE не очень разумно выдает штрафы. Поясним. Пусть объект относится к классу 𝑦=+1. Тогда:
    - на абсолютно верном предсказании 𝑎(𝑥)=1 (то есть вероятность того, что объект относится к классу +1, равна 1, поэтому получаем ошибку (штраф) (𝑎(𝑥)−𝑦)2=(1−1)2=0. Здесь всё хорошо!
    - на абсолютно неверном предсказании 𝑎(𝑥)=0 (то есть вероятность того, что объект относится к классу +1, равна 0), поэтому получаем штраф (𝑎(𝑥)−𝑦)2=(0−1)2=1.  _Очень маленький штраф за абсолютно неверное предсказание!_ За большую ошибку штраф должен быть гораздо больше, а MSE выдает всего 1.


**Функция потерь логистической регрессии - log-loss.**

Для обучения логистической регрессии хорошо подходит **_логистическая функция потерь (или же log-loss)_**.

Она вычисляет ошибку на объекте следующим образом:

log-loss = −(𝑦⋅𝑙𝑜𝑔(𝑎(𝑥))+(1−𝑦)⋅𝑙𝑜𝑔(1−𝑎(𝑥))).
Основание логарифма обычно берут равным 2, 10 или используют натуральный логарифм.


- если объект класса 𝑦=1, то второе слагаемое обнуляется (так как тогда 1−𝑦=0), и ошибка на объекте равна −𝑙𝑜𝑔(𝑎(𝑥)),   то есть равна логарифму предсказания модели (со знаком минус)
- если объект класса 𝑦=0, то обнуляется первое слагаемое (𝑦=0), и ошибка на объекте равна −𝑙𝑜𝑔(1−𝑎(𝑥)),   то есть логарифм единицы минус предсказание модели (со знаком минус)
Действительно, логистическая функция потерь очень хорошо подходит для обучения логистической регрессии, потому что:

- она выпуклая, то есть у неё один минимум, и градиентный спуск его найдет
- она корретно штрафует ошибки. Поясним. Пусть объект относится к классу 𝑦=+1. Тогда
    - на абсолютно верном предсказании 𝑎(𝑥)=1 и log-loss = −𝑙𝑜𝑔(1)=0 (ошибка 0)
    - на абсолютно неверном предсказании 𝑎(𝑥)≈0, и log-loss = −𝑙𝑜𝑔(≈0)≈+∞ (огромная ошибка)
- кроме того, log-loss обладает замечательным свойством: **модель, обученная при помощи минимизации log-loss, предсказывает корректные математические вероятности!** Это именно то, чего мы ожидаем от логистической регрессии!*

* Поясним: изначально, когда мы говорили, что логистическая регрессия делает предсказания по формуле 𝑎(𝑥)=𝜎(𝑤,𝑥)a(x)=σ(w,x), мы гарантировали только, что модель предскажет какое-то число из отрезка [0;1][0;1]. Но это число не обязано быть именно вероятностью того,  что объект относится к положительному классу.

**Метод максимума правдоподобия (ММП)

Мы пользуемся методом максимального правдоподобия. Объясним метод "на пальцах".

Введем понятие _правдоподобие выборки_. Правдоподобие выборки можно объяснить так:

_это мера того, насколько хорошо модель подходит к имеющимся данным._

Если правдоподобие выборки высокое, это означает, что модель может объяснить данные очень близко к реальности. Если правдоподобие выборки низкое, это говорит о том, что модель плохо соответствует данным и может быть неправильной или неподходящей.

_Правдоподобие выборки рассчитывается путем умножения вероятностей каждого наблюдения в выборке в соответствии с предсказаниями модели._ Чем больше это произведение, тем лучше модель объясняет данные.

- Запишем вероятность того (по мнению модели), что в выборке встретится объект 𝑥x с классом 𝑦:

$𝑎(𝑥)^{[𝑦=1]}⋅(1−𝑎(𝑥))^{[𝑦=0]}$,

где [𝑦=1]=1, если класс равен 1, и 0 иначе.

- Тогда правдоподобие выборки выглядит так:

$$Π=∏_{𝑖=1}^{𝑛}𝑎(𝑥_𝑖)^{[𝑦𝑖=1]}⋅(1−𝑎(𝑥_𝑖))^{[𝑦𝑖=0]}$$

Будем решать задачу подбора таких параметров модели 𝑎(𝑥)a(x), на которых максимизируется правдоподобие. Тогда модель будет предсказывать вероятности, наиболее близкие к правильным. В этом состоит метод максимума правдоподобия (ММП) и так обучается логистическая регрессия!