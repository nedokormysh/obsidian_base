
_**Accuracy**_ - это доля правильных ответов модели. 

𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦=1𝑙∑𝑖=1𝑙[𝑎(𝑥𝑖)=𝑦𝑖]

- где 𝑎(𝑥𝑖)- предсказанный класс на объекте 𝑥𝑖
- 𝑦𝑖- правильный ответ (класс) на объекте 𝑥𝑖
- [𝑎(𝑥𝑖)=𝑦𝑖][a(xi​)=yi​] - индикатор, то есть величина, равная 1, если 𝑎(𝑥𝑖)=𝑦𝑖, и 0 иначе.

_Accuracy_ - это простая и понятная метрика качества классификации, поэтому она часто используется на практике.

Когда классы в задаче несбалансированы (то есть объектов одного класса сильно больше, чем объектов другого класса), accuracy будет показывать завышенное качество и не будет отражать реальное качество модели! В этом случае эту метрику использовать не нужно.

У метрики accuracy есть еще один недостаток.

В тестовых данных в задаче скоринга имеется 1000 клиентов, и `accuracy = 0.8`.

Можно ли сказать, что модель имеет хорошее качество?

Нет, так как неизвестна бизнес-постановка задачи, а измерение accuracy подходит не под любые цели бизнеса

Мы не можем определить качество модели с _accuracy_ = 0.8, потому что не конкретизированы бизнес-цели.

Модель ошибается на 20% клиентов. Но как именно она ошибается?

Существует два вида ошибок:

- модель выдает кредит клиентам, которые его не вернут
- модель не выдает кредит клиентам, которые его вернут

В зависимости от бизнес-целей задачи нужно больше смотреть на какой-то один вид ошибок, или же на оба вида в совокупности. В любом случае _accuracy_ не показывает, ошибку какого вида совершает модель. Метрика лишь показывает, что модель ошибается, но как ошибается - неизвестно.

**Матрица ошибок**

При оценке качества классификации модели можно строить **_матрицу ошибок_** (**_confusion matrix_**):

![](https://ucarecdn.com/28fd5435-8b8d-4c65-a805-aec84835e65e/)

Эта матрица состоит из четырех чисел:

- _True Positive_ (_**TP**_) - количество объектов положительного класса, предсказанных моделью как положительные (верные предсказания)
- _False Positive_ (**_FP_**) - количество объектов отрицательного класса, предсказанных моделью как положительные (ошибки модели)
- _False Negative_ (**_FN_**) - количество объектов положительного класса, предсказанных моделью как отрицательные (ошибки модели)
- _True Negative_ (_**TN**_) - количество объектов отрицательного класса, предсказанных моделью как отрицательные (верные предсказания).

Матрица ошибок модели уже дает больше информации о качестве модели, чем _accuracy_, так как показывает, какие ошибки совершает модель и сколько.

Существуют две удобные метрики, с помощью которых можно измерить качество модели, используя матрицу ошибок:

- **точность (_precision_)**
- **полнота (_recall_)**

**Precision (точность)**

Оценим, насколько точна модель среди тех клиентов, которым она выдала кредит?  
То есть посчитаем долю правильных ответов модели среди всех её положительных предсказаний - это и есть метрика _precision_:

𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=𝑇𝑃𝑇𝑃+𝐹𝑃

**Recall (полнота)**

Как много кредитоспособных клиентов (тех, кто вернет кредит) находят модели?  
Среди всех клиентов, кто вернет кредит, посчитаем долю клиентов, которым модель выдала кредит - это метрика _recall_:

𝑟𝑒𝑐𝑎𝑙𝑙=𝑇𝑃𝑇𝑃+𝐹𝑁​

Иногда важно, чтобы и точность, и полнота были побольше, при этом нет отдельных требований к точности и полноте. Тогда, конечно, можно использовать _accuracy_. Но как считать качество, если данные несбалансированы? 

В этом случае измеряют некоторую усредненную по точности и полноте величину. Она называется **f1-score:**

f1=precision+recall2⋅precision⋅recall​

- _F1-score_ - это, как и точность с полнотой, величина из отрезка [0;1][0;1]. Если и точность, и полнота довольно большие, то _f1-score_ близка к 1.
- Если же одна из метрик _precision_, _recall_ или обе метрики низкие, то и _f1-score_ далека от 1.

_F1-score_ - хорошая метрика, позволяющая по одному значению оценить качество модели бинарной классификации и не волноваться о влиянии дисбаланса классов.

**Улучшение _precision_ и _recall_ (подбор порога)**

Нам осталось обсудить одну важную деталь.

На _precision_ и _recall_ (а значит, и на _f1_) можно влиять путём подбора порога перевода вероятности в классы.

Разберем подход на примере. В таблице

- в первом столбце записаны вероятности класса +1+1, предсказанные моделью
- во втором столбце - правильные ответы
- в третьем столбце - классы, предсказанные моделью, если использовать стандартный порог вероятности: все объекты с вероятностью принадлежности к положительному классу ≥0.5≥0.5, относятся к классу +1+1, а остальные объекты - относим к отрицательному классу
А именно:

- увеличивая порог, мы увеличиваем precision и уменьшаем _recall_ (так как выдаем кредиты всё меньшему числу клиентов - только тем, в ком уверены)
- уменьшая порог, мы увеличиваем _recall_ и уменьшаем _precision_ (выдаем все больше кредитов - поэтому увеличиваем полноту, но уменьшаем точность).

Подбором порога нельзя увеличить и точность, и полноту, они ведут себя обратным образом. Но можно добиться увеличения более важной для задачи метрики.