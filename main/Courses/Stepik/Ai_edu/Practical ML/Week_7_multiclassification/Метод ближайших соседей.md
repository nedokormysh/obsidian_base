
Идея метода очень простая, она называется **_гипотезой компактности_**: схожие объекты находятся близко друг к другу в пространстве признаков.

**Алгоритм метода**

У метода есть гиперпараметр _𝑘 - число соседей_.

Чтобы определить, к какому классу относится объект,  нужно:

- вычислить расстояние от объекта до каждого объекта выборки
- выбрать 𝑘 объектов выборки с наименьшим расстоянием (𝑘 ближайших соседей)
- класс искомого объекта - это наиболее часто встречающийся класс среди 𝑘 ближайших соседей.

Формально последний пункт записывается так:

если 𝑌 - множество всех возможных классов в задаче, а 𝑦𝑖​ - класс 𝑖-го объекта из найденных 𝑘 ближайших объектов к объекту 𝑞, то предсказание модели на объекте 𝑞

𝑎(𝑞)=arg max⁡𝑦∈𝑌∑𝑖=1𝑘𝐼[𝑦𝑖=𝑦],

где 𝐼[𝑦𝑖=𝑦]=1, если 𝑖-й объект принадлежит классу 𝑦y, и 0 - иначе. 

Сумма берется по 𝑘 ближайшим к объекту 𝑞 соседям.

**Влияние гиперпараметра 𝑘**

Алгоритм определения классов очень простой. На самом деле _**у метода нет фазы обучения**_, потому что нет параметров, которые подбираются в процессе обучения по выборке. В этом смысле метод очень простой.

Несмотря на свою простоту, метод легко переобучается. Например, если взять число соседей очень маленьким (𝑘=2 или 𝑘=3, метод будет делать предсказания только по двум или трем самым близким точкам, и поэтому сильно подгонится под данные. В этом можно убедиться, посмотрев на разделяющую поверхность метода при разных значениях 𝑘:

Разделяющая поверхность при маленьких значениях 𝑘 очень сложная, и это означает, что модель подстраивается под данные, что ведёт к сильному переобучению.

**Выбор метрики**

В методе ближайших соседей мы вычисляем расстояния между объектами. Способов вычислить расстояние очень много, каждый из них задается своей формулой (метрикой). Каждая метрика больше подходит для своего типа задач.

Наиболее используемые метрики:

- **Евклидова метрика** - классический способ измерить расстояние между объектами. Пусть объекты 𝑎a и 𝑏b имеют координаты 𝑎=(𝑥1,𝑦1), 𝑏=(𝑥2,𝑦2). Тогда евклидово расстояние между ними

𝜌(𝑎,𝑏)=\sqrt{(𝑥1−𝑥2)^2+(𝑦1−𝑦2)^2}

- **Манхеттенское расстояние** - другой способ посчитать расстояние между двумя точками:

𝜌(𝑎,𝑏)=∣𝑥1−𝑥2∣+∣𝑦1−𝑦2∣

- **Расстояние Хемминга** - это число различных позиций в координатах двух векторов
- Мера Жаккара - ещё один способ померить расстояние (часто используется для измерения похожести текстов). Пусть 𝐴 и 𝐵 некоторые множества, тогда мера Жаккара - это

​**Масштабирование данных для KNN**

При использовании KNN в ситуации, когда объекты описываются вектором из числовых признаков необходимо масштабировать данные. Почему так?

**Приведем пример**

Пусть мы ищем ближайшие объекты к объекту 𝑎=(40,1,100000)a=(40,1,100000), где 4040 - возраст человека, 11 - пол (1 - мужчина, 0 - женщина), 100000100000 - месячная зарплата.

Какой объект будет ближайшим к объекту 𝑎a по евклидовой метрике?

- 𝑏=(80,0,90000)
- 𝑐=(38,0,50000)
- 𝑑=(25,1,1000000)

𝜌(𝑎,𝑏)=(80−40)2+(0−1)2+(90000−100000)2=1600+1+100000000≈10000.ρ(a,b)=(80−40)2+(0−1)2+(90000−100000)2​=1600+1+100000000​≈10000.

Мы видим, что наибольший вклад в ответ вносит зарплата человека, а остальные признаки по большому счету не важны. А это не всегда так, ведь это зависит от задачи и от многих других факторов. 


**Обобщения KNN**

У классического KNN есть один большой недостаток - он никак не учитывает расстояния до ближайших объектов. В то же время понятно, что объекты, находящиеся ближе к объекту запроса, должны вносить больший вклад в ответ. Также учёт расстояний будет очень важен, когда мы будем применять KNN для решения задач регрессии.

Как можно учесть расстояния?

Напомним, что классический KNN определяет класс объекта 𝑞 как самый популярный класс среди 𝑘 его ближайших соседей:

𝑎(𝑞)=arg max⁡𝑦∈𝑌∑𝑖=1𝑘𝐼[𝑦𝑖=𝑦],

то есть все ближайшие соседи входят в формулу с весом 1. Можно изменить вес в зависимости от объекта.

- можно упорядочить соседей по увеличению расстояния от объекта запроса 𝑞q и давать им вес, обратно пропорциональный номеру соседа: 𝑤_𝑘=1/𝑘​, то есть

𝑎(𝑞)=arg max⁡𝑦∈𝑌∑𝑖=1𝑘𝐼[𝑦𝑖=𝑦]/𝑘.

Такой подход всё ещё не учитывает реальные расстояния от объекта запроса до ближайших объектов.

- способ учесть расстояния называется _методом Парзеновского окна:_

$$𝑎(𝑞)=arg max_{⁡𝑦∈𝑌}∑_{𝑖=1}^𝑘𝐾\Big(\frac{𝜌(𝑞,𝑥𝑖)}{ℎ}\Big)𝐼[𝑦_𝑖=𝑦],$$
где 𝑞 - вектор признаков запроса, 𝑥𝑖​ - вектор признаков 𝑖-го ближайшего соседа, ℎ - ширина окна.

Функция 𝐾(𝑥) называется ядром. Существует множество различных ядер, например:

- 𝐾(𝑥)=12𝐼[∣𝑥∣≤1] (прямоугольное ядро)
- 𝐾(𝑥)=(1−∣𝑥∣)⋅𝐼[∣𝑥∣≤1](треугольное ядро)
- 𝐾(𝑥)=12𝜋𝑒−2𝑥2 (гауссовское ядро)

**KNN в задаче регрессии**

С помощью KNN можно решать не только задачи классификации, но и задачи регрессии. Существует как простой, так и более сложные подходы - все они полностью аналогичны подходам в задачах классификации:

- Простой вариант:
 𝑎(𝑞)=1𝑘∑𝑖=1𝑘𝑦𝑖,
 ​,то есть предсказанный ответ это просто среднее арифметическое целевых переменных 𝑘 ближайших соседей
- Взвешенный вариант (формула Надарая-Ватсона):

𝑎(𝑞)=∑𝑖=1𝑘𝐾(𝜌(𝑞,𝑥𝑖)/ℎ)𝑦𝑖/∑𝑖=1𝑘𝐾(𝜌(𝑞,𝑥𝑖)/ℎ).

На результат предсказания, как и в задаче классификации, влияет и число соседей, и выбор формулы для предсказания.

**Преимущества и недостатки KNN**

Преимущества KNN:

- Простой алгоритм (для объяснения и для интерпретации)
- Метод не делает никаких предположений о данных (об их линейной разделимости, о распределении данных)
- В некоторых задачах достаточно хорошо работает
- Применяется и для классификации, и для регрессии

Недостатки KNN:

- Требует больших ресурсов по памяти, так как хранит всю выборку
- Требует больших ресурсов по времени, так как вычисляет расстояния до всех объектов выборки
- Чувствителен к масштабу данных
- Зависит от выбранной метрики, которая в свою очередь должна отражать реальное сходство объектов. Найти такую метрику не всегда просто или даже возможно