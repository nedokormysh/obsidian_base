

**Support Vector Machine**


Будем разбирать метод опорных векторов, двигаясь от простого к сложному. Наш план такой:

- SVM для линейно разделимой выборки.
- SVM для линейно неразделимой выборки.
- Обобщения SVM для решения нелинейных задач.

**Первый случай. Линейно разделимая выборка**

Линейно разделимая выборка в задаче бинарной классификации - это набор объектов двух классов, которые можно безошибочно классифицировать, разбив пространство на две части с помощью линейной поверхности (гиперплоскости).

В случае линейно разделимой выборки существует бесконечно много прямых (а значит, и бесконечно много классификаторов, каждый из которых задает свою прямую), безошибочно решающих задачу классификации.

**Отступ (margin)**

Так как вариантов построить классификатор, не допускающий ошибок на линейно разделимых данных, много, то давайте строить _самый уверенный_ классификатор! Самый уверенный классификатор - это тот классификатор, который наиболее уверен в предсказанных классах на объектах выборки. Как же измерить эту уверенность? 

Чтобы ответить на этот вопрос, введём понятие отступа (margin) на объекте. Отступ 𝑀𝑖​ на объекте 𝑥𝑖​ вычисляется по формуле

𝑀𝑖=𝑦𝑖⋅(𝑤,𝑥𝑖),,где

- 𝑦𝑖yi​ - правильный ответ (класс, +1 или -1)
- (𝑤,𝑥𝑖)​) - скалярное произведение вектора весов модели и вектора признаков.

У отступа есть вполне понятный геометрический смысл.

**Факт №1:** _знак отступа говорит о корректности классификации объекта._
Если **класс объекта определен моделью верно,** то на этом объекте **𝑀>0.**  
Если же **классификатор ошибся,** то **𝑀<0.


**Факт №2:** _абсолютная величина отступа говорит об уверенности модели в предсказании на объекте_

Таким образом, по знаку и абсолютной величине отступа можно судить:

1) о корректности классификации на объекте,
2) об уверенности модели в предсказании на объекте.

Самая уверенная модель - это та, для которой объекты выборки находятся на максимальном расстоянии от разделяющей прямой, то есть имеют _наибольшие_ отступы. Другими словами, это модель, при которой все объекты выборки находятся достаточно далеко от разделяющей прямой.

**Разделяющая полоса**

Продолжим рассматривать случай линейно разделимой выборки. Вокруг разделяющей прямой любого классификатора есть пространство, в которое не попадает ни один объект выборки. Другими словами, можно провести полосу _максимальной ширины_ вокруг разделяющей прямой, которая не содержит в себе ни одного объекта (на границе этой полосы будут какие-то объекты). Эта полоса называется **разделяющей полосой**.

По сути, разделяющая полоса - это своего рода _полоса уверенности._ Классификатор уверен в предсказаниях для всех объектов, которые находятся вне этой полосы, поскольку они удалены от разделяющей прямой.

В терминах разделяющей полосы можно сказать, что _**метод опорных векторов (SVM) - это классификатор, имеющий наиболее широкую разделяющую полосу среди всех возможных линейных классификаторов!**_

Сформулируем задачу метода SVM с математической точки зрения:

SVM делает предсказания классов по формуле **𝑎(𝑥)=𝑠𝑖𝑔𝑛(𝑤,𝑥).**

Задача обучения SVM в линейно разделимом случае имеет вид:

$\frac{1}{2}∣∣𝑤∣∣^2→min$​
при условии, что для всех объектов выборки выполняется неравенство: 𝑦𝑖⋅(𝑤, 𝑥𝑖)≥1, для всех 𝑖.

Здесь 𝑦𝑖— это истинные метки классов, 𝑥𝑖 — это объекты выборки.

**Второй случай. Линейно неразделимая выборка**

В большинстве задач линейной разделимости в данных нет. Эти задачи можно разделить на два случая:

- Есть линейная зависимость в данных: классы можно в принципе разделить прямой линией, но из-за шума в данных строгая линейная разделимость отсутствует.
- Нет линейной зависимости в данных: классы не могут быть разделены прямой линией.

Метод опорных векторов можно применять в обоих случаях, при этом для каждого из них существует своя модификация метода.

_Начнем со случая, когда линейная зависимость в данных есть, но мы не можем безошибочно классифицировать объекты._

**Линейно неразделимая выборка**

Будем рассматривать ситуацию, когда в данных есть линейная зависимость, но линейной разделимости нет.

Для решения этой задачи мы всё так же хотим использовать SVM, то есть строить классификатор с наиболее широкой разделяющей полосой. Но _в этом случае в разделяющую полосу всегда будут попадать какие-то объекты (ведь линейной разделимости нет)!_

Будем **штрафовать** объекты, попадающие внутрь разделяющей полосы (так как это объекты, в которых модель не уверена).

При обучении SVM в линейно неразделимом случае решаются две задачи:

- максимизируется ширина разделяющей полосы;
- минимизируется сумма штрафов на объектах выборки.

С точки зрения математики функция потерь SVM в этом случае имеет вид

$\frac{1}{2}​∣∣w∣∣^2+C\sum_{i=1}^l​ξ_i​→wmin$

где 𝐶 - гиперпараметр, регулирующий силу штрафов, а 𝜉𝑖​ - штраф на 𝑖-м объекте выборки.

**Гиперпараметр С**

Метод позволяет регулировать то, насколько сильно мы хотим максимизировать ширину полосы или же нам важнее получить меньше штрафуемых объектов.

Тогда в зависимости от значения C происходят следующие изменения:

- при маленьких 𝐶 нам не важно, как ведут себя штрафы, и метод фокусируется на построении алгоритма с наиболее широкой разделяющей полосой;
- при больших 𝐶, наоборот, нам важно минимизировать штрафы, поэтому и разделяющая полоса будет узкой.

**Опорные вектора**

Объекты, попадающие на границу или внутрь разделяющей полосы, а также объекты, попадающие не в свой класс, называются **_опорными векторами_**.

SVM старается минимизировать количество таких объектов, то есть при обучении ориентируется именно на них.