# Методы отбора признаков

- Данных может быть слишком много, и модель не обучится на них из-за размера и длительного времени обучения. В этом случае для уменьшения размерности данных можно выкидывать признаки
- Некоторые признаки могут быть шумовыми - то есть никак не связаны с задачей, а иногда такие признаки будут даже усложнять модели поиск зависимостей в данных - в этом случае признаки надо удалять
- Если признаки в задаче линейно-зависимы, то необходимо удалять некоторые из таких признаков, чтобы модели не переобучались

#**Виды отбора признаков**

Сейчас мы обсудили фильтрационные методы отбора признаков. Все алгоритмы отбора можно поделить на три большие группы:

- фильтрационные методы
- оберточные методы
- отбор признаков при помощи ML-моделей
### **Variance Threshold**

Есть признаки, которые почти на всех объектах принимают одно и то же значение - то есть почти везде константы.  
Такие признаки нам точно не нужны в данных, ведь они не несут никакой информации. 

С точки зрения математики эти признаки имеют очень маленькую дисперсию, и их можно спокойно удалять.

### **Фильтрационные методы отбора признаков**

На предыдущем шаге мы считали корреляцию, но существуют и другие функции, подсчет которых показывает силу взаимосвязи между столбцами (между признаком и целевой переменной).

К фильтрационным методам относятся, например:

- вычисление корреляции
- вычисление статистики 𝜒2
- вычисление взаимной информации

Про пункты 2 и 3 поговорим подробнее.

#### **Отбор по корреляции с целевой переменной**

Для каждого признака вычислим его корреляцию с целевой переменной. Будем выкидывать признаки, имеющие маленькую по модулю корреляцию.

Этот подход хорош тем, что работает быстро: мы вычисляем столько значений корреляций, сколько признаков в данных, и затем удаляем признаки с наименьшей по модулю корреляцией с целевой переменной.

К недостаткам подхода можно отнести то, что бывают ситуации, когда сам по себе признак имеет слабую корреляцию с ответом, а два признака в совокупности (или больше) очень сильно влияют на ответ. В этом случае если мы удалим такие слабо коррелирующие с таргетом признаки, то у модели не будет шанса хорошо предсказать целевую переменную.

#### **Отбор признаков по 𝜒2 в задаче классификации**

Этот способ работает для отбора категориальных признаков в задаче классификации.

Статистика 𝜒2 используется для того, чтобы оценить взаимосвязь между признаками. Она иногда используется как аналог корреляции, но не между парой числовых признаков, а между парой категориальных признаков. На основе этой статистики можно также делать отбор признаков.

Пусть 𝑋,𝑌 - категориальный признак и целевая переменная (в задаче классификации), тогда 𝜒2вычисляется по формуле

$𝜒^2(𝑋,𝑌)=\frac{∑_{𝑖,𝑗}(𝑂𝑖𝑗−𝐸𝑖𝑗)^2}{𝐸_{𝑖𝑗}}$

где 𝑂𝑖𝑗​ - наблюдаемая частота события, 𝐸𝑖𝑗​ - ожидаемая частота события (в предположении, что величины 𝑋 и 𝑌 независимы).

_Чем больше значение 𝜒2- тем больше степень взаимосвязи переменных 𝑋 и 𝑌.

#### **Отбор признаков по взаимной информации**

Этот способ отбора подходит для числовых признаков.

Взаимная информация (mutual information) для числовых векторов 𝑋 и 𝑌 вычисляется по формуле

$𝐼(𝑋,𝑌)=∑_{𝑦∈𝑌}∑_{𝑥∈𝑋}𝑝(𝑥,𝑦)𝑙𝑜𝑔\Big(\frac{𝑝(𝑥,𝑦)}{𝑝(𝑥)𝑝(𝑦)}\Big)$

Здесь

- 𝑝(𝑥,𝑦) - совместная плотность вероятности 𝑋 и 𝑌
- 𝑝(𝑥) - плотность вероятности признака 𝑋
- 𝑝(𝑦) - плотность вероятности целевой переменной 𝑌.

Взаимная информация показывает сколько информации о признаке 𝑋 содержится в ответе 𝑌.

Как производится отбор признаков по взаимной информации:

- вычисляем 𝐼(𝑋,𝑌) для каждой пары признак-ответ
- выкидываем признаки с наименьшими значениями 𝐼(𝑋,𝑌).


## **Оберточные методы**

Оберточные методы используют _жадный отбор признаков_, то есть последовательно выкидывают наименее подходящие по мнению методов признаки.

Пример работы оберточного метода:

Выбираем модель, например, случайный лес.

Пусть в данных 300 признаков, а наша цель - оставить в данных 100 самых информативных признаков, либо выкинуть столько признаков, без которых качество модели увеличивается (по сравнению с моделью, обученной на исходном наборе признаков).

- На первом шаге обучаем модель 300 раз: первый раз - на всех признаках, кроме первого, второй - на всех признаках кроме второго и так далее. Затем выкидываем тот признак, без которого качество модели наибольшее.
- На следующем шаге обучаем модель уже 299 раз - на всех оставшихся признаках, кроме первого, затем - на всех оставшихся признаках кроме второго и так далее. Выкидываем признак, без которого качество модели наибольшее.
- И так далее до тех пор, пока не останется требуемое число признаков (100), либо до тех пор, пока при выкидывали признаков качество модели растет.

**Фильтрационные методы vs Оберточные методы**

- Фильтрационные методы быстрее, так как для отбора нужно вычислить 𝑁N значений некоторой функции (корреляция, 𝜒2χ2 и так далее), где 𝑁N - число признаков
- Оберточные методы точнее, так как они учитывают влияние взаимосвязи признаков на ответ (а фильтрационные методы - только влияние каждого отдельного признака на ответ)


## **Встроенные в модель методы отбора**

Вспомним про свойство _L1-регуляризации_: она не только уменьшает веса модели, но и зануляет некоторые веса!

𝑄(𝑤)+𝛼∑𝑗=1𝑑∣𝑤𝑗∣→𝑚𝑖𝑛𝑤

Поэтому при обучении моделей с L1-регуляризацией мы убиваем сразу двух зайцев:

- решаем исходную задачу
- одновременно с этим избавляемся от ненужных признаков 

Для отбора признаков также (но гораздо реже) используют _L0-регуляризацию_: она штрафует модель за большое количество ненулевых весов.

𝑄(𝑤)+𝛼∑𝑗=1𝑑[𝑤𝑗≠0]→𝑚𝑖𝑛𝑤.

Здесь [𝑤𝑗≠0]=1=1, если 𝑤𝑗≠0, и 0иначе.

При обучении моделей с L0-регуляризацией также происходит отбор признаков.

Иногда при помощи моделей отбирают признаки другим способом:

- Сначала обучают модель (линейную, дерево, лес, бустинг и так далее)
- Затем смотрят на важности признаков (веса - у линейных моделей, feature importances - у деревьев и их композиций)
- Наконец, выкидывают признаки с наименьшей важностью

Встроенные методы довольно часто работают хорошо и достаточно быстро, так как при обучении моделей мы учитываем и совокупное влияние признаков на ответ, и при этом получаем как результат важность признаков.



[[Practical_ML_content]] [[00_ML]] 