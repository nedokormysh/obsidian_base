Напомним:

- Функция потерь - это функция, которая минимизируется в процессе обучения модели (по-другому она называется _loss_)
- Метрика качества - это функция, которая используется для оценки качества уже готовой (обученной) модели

Иногда одну и ту же функцию используют и в роли лосса, и как метрику качества. Это не страшно. 

Более того, иногда в разговорах метрикой называют и лосс, и метрику качества - и это тоже считается нормальным в среде специалистов.

# **Среднеквадратичная ошибка**

Начнём с самой известной и самой популярной метрики (она используется и как _loss_, и как метрика качества) - **среднеквадратичной ошибки (MSE - mean squared error)**:

$𝑀𝑆𝐸=\frac{1}{l}∑_{𝑖=1}^𝑙(𝑎(𝑥_𝑖)−𝑦_𝑖)^2$

- 𝑎(𝑥𝑖)- предсказание модели на объекте 𝑥𝑖xi​
- 𝑦𝑖​ - правильный ответ
- 𝑙- число объектов в выборке, на которой вычисляется значение метрики

Если использовать MSE как метрику, то у неё есть два недостатка:

- Во-первых, она не сохраняет размерность: кг→кг2,руб→руб2 и так далее.

Поэтому на практике используют RMSE (root mean squared error) - корень из MSE, чтобы сохранить размерность:

$𝑅𝑀𝑆𝐸=\sqrt{\frac{1}{𝑙}∑_{𝑖=1}^𝑙(𝑎(𝑥_𝑖)−𝑦_𝑖)^2}$

## Проблема MSE

Следующая проблема MSE такая: 
представьте себе, что

𝑅𝑀𝑆𝐸=10000(рублей)RMSE=10000(рублей)

на тестовой выборке. Это большая ошибка или нет? Непонятно.

Чтобы понять, большая это ошибка или нет, необходимо посмотреть, а что мы собственно предсказываем. Если мы предсказываем величину чека клиента в ресторане - это большая ошибка, а если стоимость машины - маленькая.

Неудобно, что для интерпретации метрики нам необходимы дополнительные знания о задаче. Хотелось бы, чтобы по значению метрики можно было бы сразу сказать, хорошая модель или плохая.

И такая метрика есть - она называется 𝑅2R2, читайте дальше!

## **R2 (коэффициент детерминации)**

Метрика 𝑅2 - это не ошибка модели, это мера качества! 

$$𝑅^2=1−\frac{∑_{𝑖=1}^𝑙(𝑎(𝑥_𝑖)−𝑦_𝑖)^2}{∑_{𝑖=1}^𝑙(𝑦_𝑖−𝑦‾)^2}$$

где $𝑦‾=\frac{1}{𝑙}∑_{𝑖=1}^𝑙𝑦_𝑖$ - среднее значение целевой переменной.

Заметим, что в числителе вычитаемой дроби стоит MSE (без деления на 𝑙), а в знаменателе - дисперсия целевой переменной (без деления на 𝑙). То есть эту метрику можно интерпретировать как **_долю дисперсии целевой переменной, объясняемую моделью._**

Если говорить по-простому, то в знаменателе дроби стоит константа (число, не зависящее от модели), а в числителе - MSE. И зависимость такая:

- Для идеальной модели (MSE=0) 𝑅2=1
- Чем хуже модель (чем больше MSE), тем меньше 𝑅2.
R2 может быть и отрицательным - в случае, если модель предсказывает хуже, чем 𝑎(𝑥)=𝑦‾​, то есть чем модель, предсказывающая средний ответ на всех объектах. То есть отрицательный 𝑅2 говорит о том, что модель очень плохо решает задачу (или сильно переобучена).

## **Средняя абсолютная ошибка**

Только что вы узнали о трех метриках: MSE, RMSE и 𝑅2. На самом деле это всё - вариации среднеквадратичной ошибки (MSE).

Но существуют и другие метрики. Например, очень популярна наряду с MSE метрика **MAE (mean absolute error, средняя абсолютная ошибка)**:

$𝑀𝐴𝐸=\frac{1}{𝑙}∑_{𝑖=1}^𝑙∣𝑎(𝑥_𝑖)−𝑦_𝑖∣,$

где 

- 𝑎(𝑥𝑖)- предсказание модели на объекте 𝑥𝑖​
- 𝑦𝑖​ - правильный ответ
- 𝑙 - число объектов в выборке, на которой считается MAE.

Метрика MAE в отличие от MSE сохраняет размерность, то есть килограммы остаются килограммами, а рубли - рублями.

Но MAE так же неограничена, как и MSE.

## **MAPE, SMAPE**

Существуют варианты нормировки MAE, их довольно много: MAPE, SMAPE, WAPE и другие.

Расскажем про первые два:

$$𝑀𝐴𝑃𝐸=\frac{1}{𝑙}∑_{𝑖=1}^𝑙\frac{∣𝑦_𝑖−𝑎(𝑥_𝑖)∣}{∣𝑦_𝑖∣}$$

Эта метрика (**MAPE - mean average percentage error**) показывает на сколько процентов в среднем ошибается наша модель. То есть, если 𝑀𝐴𝑃𝐸=0.2, то это значит, что модель в среднем ошибается на 20% относительно правильных ответов.

Всё бы хорошо, но эта метрика несимметрична. Поясним:

Пусть предсказание модели на некотором объекте равно 𝑎(𝑥)=3, а правильный ответ:

- 𝑦 = 5. Тогда ошибка на этом объекте ∣5−3∣/∣5∣=0.4
- 𝑦 = 1. Тогда ошибка на этом объекте ∣1−3∣/∣1∣=2

Получается, что в обоих случаях модель ошиблась на 2 единицы, но ошибка при этом разная. Это нехорошо, то есть говорит о несимметричности метрики, то есть метрика меньше штрафует занижение прогноза, чем перепрогноз.

Несимметричность попытались исправить симметричной версией метрики MAE:

$$𝑆𝑀𝐴𝑃𝐸=\frac{1}{𝑙}∑_{𝑖=1}{𝑙}\frac{∣𝑦_𝑖−𝑎(𝑥_𝑖)∣}{(∣𝑦_𝑖∣+∣𝑎(𝑥_𝑖)∣)/2}$$

**SMAPE (symmetric mean average percentage error)** пытается сделать прогноз симметричным, но этого не происходит (можете проверить на том же примере, что и выше), однако формула для оценки качества становится сложнее и менее интерпретируемой. Поэтому на практике эту метрику используют нечасто.



Метрика качества обычно выбирается, исходя из особенностей задачи.

Например, если данные грязные (в них есть выбросы) и нет возможности их почистить (или есть другие причины этого не делать), то для оценки качества используют MAE, а не MSE - так как MAE более устойчива к выбросам,  чем среднеквадратичная ошибка.

Если же в задаче опаснее недопрогнозировать, чем перепрогнозировать (или наоборот) - то используют несимметричные метрики, например, MSLE (среднеквадратичную логарифмическую ошибку).
