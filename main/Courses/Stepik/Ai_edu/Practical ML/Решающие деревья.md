
Решающие деревья - это деревья (как математический объект), то есть ориентированные графы с _корнем_ и несколькими концевыми вершинами (_листьями_).

При помощи решающих деревьев можно описать довольно много алгоритмов. Часто схема работы экспертов и различных программ описана именно деревом.

# **Терминология**

- **Корень дерева** - вершина, из которой исходят ребра, но не входит ни одно ребро
- **Ребра -** отрезки, исходящие из вершин дерева
- **Предикат -** условие, которое проверяется в вершине дерева
- **Лист** - концевая вершина дерева; в листьях дерева стоят предсказания модели

Деревья бывают разные, но для целей _Data Science_ чаще всего используются _бинарные решающие деревья_, то есть деревья, из каждой внутренней (не листовой) вершины исходит ровно две ветки.

**Предикаты**

Предикаты, то есть условия, которые мы проверяем в вершинах дерева, могут быть разными. Но в классических решающих деревьях предикаты очень простые: Предикат имеет вид **"признак > порога"**, то есть в каждой вершине используется ровно один признак и он сравнивается с некоторым пороговым значением.

**Жадный алгоритм**

Деревья на практике строятся пошагово, то есть сначала подбирается первый предикат, который дает наилучшее качество разбиения. Затем второй, третий и так далее. Такой алгоритм построения дерева называется _жадным_.

Жадный алгоритм - не самый оптимальный. Гораздо лучше было бы перебирать всевозможные готовые деревья и выбирать из них наилучшее, но это очень сложная задача (NP-полная) и за разумное время её решить невозможно.

Важной особенностью деревьев - **_они склонны к сильному переобучению!_**
Поэтому чтобы дерево не переобучалось, существуют подходы к ограничению сложности его структуры: ограничение глубины дерева, ограничения на количество объектов в промежуточных вершинах и в листьях дерева и другие.

# **Обучение решающих деревьев**

На практике оптимальное по структуре дерево подбирается автоматически, исходя из минимизации некоторой функции ошибки. Сейчас мы разберемся, что это за функция ошибки. 

Чтобы ввести функцию ошибки, сначала нужно понять, чего мы хотим при построении дерева? Разберем идею построения дерева на примере задаче бинарной классификации

Идея построения дерева:

Пусть в вершине дерева есть объекты двух классов,  и их в вершине поровну. Это максимально перемешанная выборка, так как, находясь в этой вершине, мы видим объекты разных классов,  причем в равных количествах.

Наша задача - подобрать такой предикат, который разобъет эту выборку на две части идеально: так, что в одну вершину попадут объекты одного класса, а в другую - другого (то есть безошибочно решить задачу классификации).

Скорее всего мы не найдем сразу такой предикат, который сразу идеально разобъет объекты на классы. Но мы к этому стремимся.

Наша цель: найти такой предикат, то есть такую пару (признак,порог), что после разбиения по предикату признак>порога

объекты в вершинах 𝑅𝑙​ и 𝑅𝑟 максимально однородные (одного класса в каждой вершине).

**Критерий информативности**

Меру неоднородности объектов в вершине нужно как-то измерять. Для этого придумали различные функции, которые называются _критериями информативности 𝐻(𝑅).

Чем более неоднородная выборка в вершине - тем больше значение функции 𝐻(𝑅). Если выборка состоит из объектов одного класса, то это максимально однородная выборка, и для неё 𝐻(𝑅)=0.

Теперь в терминах критерия информативности мы можем уточнить задачу построения дерева.

Задача: на каждом шаге построения решающего дерева мы ищем такой предикат 𝑥𝑗>𝑡xj​>t, что после разбиения по этому предикату значение критерия информативности в каждой из полученных вершин минимально:

𝐻(𝑅𝑙)→𝑚𝑖𝑛,  𝐻(𝑅𝑟)→𝑚𝑖𝑛

решать две задачи оптимизации одновременно сложно.

Можно свести их к одной:

𝐻(𝑅𝑙)+ 𝐻(𝑅𝑟)→𝑚𝑖𝑛.

Осталось учесть одну деталь. Представим себе, что в вершине 𝑅R 100 объектов,  то есть ∣𝑅∣=100. И пусть некоторым условием мы разбили вершину на две части размеров ∣𝑅𝑙∣=90 и ∣𝑅𝑟∣=10. Очевидно, левая вершина весит гораздо больше, и верно классифицировать объекты в ней для нас весомее, чем в правой. 

Поэтому в задачу оптимизации надо добавить веса вершин.

Итак, итоговая задача формулируется так:

**мы ищем такой признак 𝑥𝑗​ и такой порог 𝑡 для него, что разбиение объектов по предикату 𝑥𝑗>𝑡 минимизирует взвешенную сумму критериев информативности. Другими словами, решаем задачу**

$\frac{∣𝑅𝑙∣}{∣𝑅∣}𝐻(𝑅_𝑙)+\frac{∣𝑅𝑟∣}{∣𝑅∣}𝐻(𝑅_𝑟)→𝑚𝑖𝑛_{𝑥_{𝑗,𝑡}}$

**Information Gain (прирост информации)**
Иногда вместо задачи минимизации решается эквивалентная ей задача максимизации:

**ищем такой признак 𝑥𝑗​ и такой порог 𝑡 для него, что разбиение объектов по предикату 𝑥𝑗>𝑡 минимизирует взвешенную сумму критериев информативности. Другими словами, решаем задачу**

$𝐼𝐺=𝐻(𝑅)−\frac{∣𝑅_𝑙∣}{∣𝑅∣}𝐻(𝑅_𝑙)−\frac{∣𝑅_𝑟∣}{∣𝑅∣}𝐻(𝑅_𝑟)→𝑚𝑎𝑥_{𝑥_𝑗,𝑡}$

Максимизируемая функция IG (Infomation Gain) - это разность перемешанности объектов до разбиения и после разбиения. Она по сути означает то,  насколько мы упорядочили объекты при помощи разбиения по предикату. Она называется прирост информации или Information Gain.

Так как 𝐻(𝑅)H(R) константа, то максимизация Information Gain и минимизация исходной функции потерь - это эквивалентные задачи.

**H(R) в задаче жесткой классификации**

Напомним, что в задаче жесткой классификации алгоритм предсказывает классы (не их вероятности, а просто классы).

𝐻(𝑅) - это мера разнородности (хаотичности) объектов с точки зрения их целевой переменной в вершине 𝑅.

_разумно считать меру неоднородности как долю объектов, определенных неверно (долю объектов не самого популярного класса) - по-другому это просто **ошибка классификации**._

**Итак, в задаче жесткой классификации критерий информативности 𝐻H - это ошибка классификации.**

**H(R) в задаче регрессии**

В задаче регрессии неоднородность в вершине - это мера различия целевой переменной на объектах в вершине (в общем-то, как и в задаче классификации).

В таких задачах неоднородность - это по сути разброс значений целевой переменной. Чем больше разброс, тем больше неоднородность, и наоборот. На математическом языке разброс - это дисперсия:  
$𝐻(𝑅)=𝐷(𝑅)=\frac{1}{n}∑_{𝑖=1}^𝑛(𝑦_𝑖−𝑚𝑒𝑎𝑛(𝑦))^2$


**H(R) в задаче мягкой классификации**

В задаче мягкой классификации, когда алгоритм на выходе выдает для каждого объекта вектор вероятностей, задача обучения дерева не настолько очевидна, как в двух предыдущих случаях.

Хорошее дерево делает максимально уверенные предсказания, поэтому функция, которую мы оптимизируем в этой задаче, должна выдавать нулевую ошибку за максимально уверенное предсказание и наибольшую ошибку за максимально неуверенное предсказание (например, когда в вершине поровну объектов разных классов).

**H(R) в задаче мягкой классификации**

 Такие функции есть:

- Энтропия:

𝐻(𝑅)=−∑𝑘=1𝐾𝑝𝑘⋅𝑙𝑜𝑔(𝑝𝑘),

где 𝑝𝑘pk​ - доля объектов 𝑘k-го класса в вершине,  а 𝐾K - число классов в задаче.

Энтропия обладает тем свойством, которое мы хотим: 𝐻(𝑅)=0H(R)=0 в ситуации, когда одно значение 𝑝𝑖=1pi​=1, а все остальные 0. При этом она принимает наибольшее значение в ситуации, когда классы перемешаны.

- Критерий Джини:

𝐻(𝑅)=∑𝑘=1𝐾𝑝𝑘⋅(1−𝑝𝑘)

где 𝑝𝑘​ - доля объектов k-го класса в вершине,  а 𝐾K - число классов в задаче.

Критерий Джини также отвечает нашим запросам: 𝐻(𝑅)=0H(R)=0 в случае, когда в вершине все объекты одного класса, и 𝐻(𝑅)H(R) максимален в случае максимальной перемешанности объектов.



[[Practical_ML_content]]