
Решающие деревья - это деревья (как математический объект), то есть ориентированные графы с _корнем_ и несколькими концевыми вершинами (_листьями_).

При помощи решающих деревьев можно описать довольно много алгоритмов. Часто схема работы экспертов и различных программ описана именно деревом.

# **Терминология**

- **Корень дерева** - вершина, из которой исходят ребра, но не входит ни одно ребро
- **Ребра -** отрезки, исходящие из вершин дерева
- **Предикат -** условие, которое проверяется в вершине дерева
- **Лист** - концевая вершина дерева; в листьях дерева стоят предсказания модели

Деревья бывают разные, но для целей _Data Science_ чаще всего используются _бинарные решающие деревья_, то есть деревья, из каждой внутренней (не листовой) вершины исходит ровно две ветки.

**Предикаты**

Предикаты, то есть условия, которые мы проверяем в вершинах дерева, могут быть разными. Но в классических решающих деревьях предикаты очень простые: Предикат имеет вид **"признак > порога"**, то есть в каждой вершине используется ровно один признак и он сравнивается с некоторым пороговым значением.

**Жадный алгоритм**

Деревья на практике строятся пошагово, то есть сначала подбирается первый предикат, который дает наилучшее качество разбиения. Затем второй, третий и так далее. Такой алгоритм построения дерева называется _жадным_.

Жадный алгоритм - не самый оптимальный. Гораздо лучше было бы перебирать всевозможные готовые деревья и выбирать из них наилучшее, но это очень сложная задача (NP-полная) и за разумное время её решить невозможно.

Важной особенностью деревьев - **_они склонны к сильному переобучению!_**
Поэтому чтобы дерево не переобучалось, существуют подходы к ограничению сложности его структуры: ограничение глубины дерева, ограничения на количество объектов в промежуточных вершинах и в листьях дерева и другие.

# **Обучение решающих деревьев**

На практике оптимальное по структуре дерево подбирается автоматически, исходя из минимизации некоторой функции ошибки. Сейчас мы разберемся, что это за функция ошибки. 

Чтобы ввести функцию ошибки, сначала нужно понять, чего мы хотим при построении дерева? Разберем идею построения дерева на примере задаче бинарной классификации

Идея построения дерева:

Пусть в вершине дерева есть объекты двух классов,  и их в вершине поровну. Это максимально перемешанная выборка, так как, находясь в этой вершине, мы видим объекты разных классов,  причем в равных количествах.

Наша задача - подобрать такой предикат, который разобъет эту выборку на две части идеально: так, что в одну вершину попадут объекты одного класса, а в другую - другого (то есть безошибочно решить задачу классификации).

Скорее всего мы не найдем сразу такой предикат, который сразу идеально разобъет объекты на классы. Но мы к этому стремимся.

Наша цель: найти такой предикат, то есть такую пару (признак,порог), что после разбиения по предикату признак>порога

объекты в вершинах 𝑅𝑙​ и 𝑅𝑟 максимально однородные (одного класса в каждой вершине).

**Критерий информативности**

Меру неоднородности объектов в вершине нужно как-то измерять. Для этого придумали различные функции, которые называются _критериями информативности 𝐻(𝑅).

Чем более неоднородная выборка в вершине - тем больше значение функции 𝐻(𝑅). Если выборка состоит из объектов одного класса, то это максимально однородная выборка, и для неё 𝐻(𝑅)=0.

Теперь в терминах критерия информативности мы можем уточнить задачу построения дерева.

Задача: на каждом шаге построения решающего дерева мы ищем такой предикат 𝑥𝑗>𝑡xj​>t, что после разбиения по этому предикату значение критерия информативности в каждой из полученных вершин минимально:

𝐻(𝑅𝑙)→𝑚𝑖𝑛,  𝐻(𝑅𝑟)→𝑚𝑖𝑛

решать две задачи оптимизации одновременно сложно.

Можно свести их к одной:

𝐻(𝑅𝑙)+ 𝐻(𝑅𝑟)→𝑚𝑖𝑛.

Осталось учесть одну деталь. Представим себе, что в вершине 𝑅R 100 объектов,  то есть ∣𝑅∣=100. И пусть некоторым условием мы разбили вершину на две части размеров ∣𝑅𝑙∣=90 и ∣𝑅𝑟∣=10. Очевидно, левая вершина весит гораздо больше, и верно классифицировать объекты в ней для нас весомее, чем в правой. 

Поэтому в задачу оптимизации надо добавить веса вершин.

Итак, итоговая задача формулируется так:

**мы ищем такой признак 𝑥𝑗​ и такой порог 𝑡 для него, что разбиение объектов по предикату 𝑥𝑗>𝑡 минимизирует взвешенную сумму критериев информативности. Другими словами, решаем задачу**

$\frac{∣𝑅𝑙∣}{∣𝑅∣}𝐻(𝑅_𝑙)+\frac{∣𝑅𝑟∣}{∣𝑅∣}𝐻(𝑅_𝑟)→𝑚𝑖𝑛_{𝑥_{𝑗,𝑡}}$

**Information Gain (прирост информации)**
Иногда вместо задачи минимизации решается эквивалентная ей задача максимизации:

**ищем такой признак 𝑥𝑗​ и такой порог 𝑡 для него, что разбиение объектов по предикату 𝑥𝑗>𝑡 минимизирует взвешенную сумму критериев информативности. Другими словами, решаем задачу**

$𝐼𝐺=𝐻(𝑅)−\frac{∣𝑅_𝑙∣}{∣𝑅∣}𝐻(𝑅_𝑙)−\frac{∣𝑅_𝑟∣}{∣𝑅∣}𝐻(𝑅_𝑟)→𝑚𝑎𝑥_{𝑥_𝑗,𝑡}$

Максимизируемая функция IG (Infomation Gain) - это разность перемешанности объектов до разбиения и после разбиения. Она по сути означает то,  насколько мы упорядочили объекты при помощи разбиения по предикату. Она называется прирост информации или Information Gain.

Так как 𝐻(𝑅)H(R) константа, то максимизация Information Gain и минимизация исходной функции потерь - это эквивалентные задачи.

**H(R) в задаче жесткой классификации**

Напомним, что в задаче жесткой классификации алгоритм предсказывает классы (не их вероятности, а просто классы).

𝐻(𝑅) - это мера разнородности (хаотичности) объектов с точки зрения их целевой переменной в вершине 𝑅.

_разумно считать меру неоднородности как долю объектов, определенных неверно (долю объектов не самого популярного класса) - по-другому это просто **ошибка классификации**._

**Итак, в задаче жесткой классификации критерий информативности 𝐻H - это ошибка классификации.**

**H(R) в задаче регрессии**

В задаче регрессии неоднородность в вершине - это мера различия целевой переменной на объектах в вершине (в общем-то, как и в задаче классификации).

В таких задачах неоднородность - это по сути разброс значений целевой переменной. Чем больше разброс, тем больше неоднородность, и наоборот. На математическом языке разброс - это дисперсия:  
$𝐻(𝑅)=𝐷(𝑅)=\frac{1}{n}∑_{𝑖=1}^𝑛(𝑦_𝑖−𝑚𝑒𝑎𝑛(𝑦))^2$


**H(R) в задаче мягкой классификации**

В задаче мягкой классификации, когда алгоритм на выходе выдает для каждого объекта вектор вероятностей, задача обучения дерева не настолько очевидна, как в двух предыдущих случаях.

Хорошее дерево делает максимально уверенные предсказания, поэтому функция, которую мы оптимизируем в этой задаче, должна выдавать нулевую ошибку за максимально уверенное предсказание и наибольшую ошибку за максимально неуверенное предсказание (например, когда в вершине поровну объектов разных классов).

**H(R) в задаче мягкой классификации**

 Такие функции есть:

- Энтропия:

𝐻(𝑅)=−∑𝑘=1𝐾𝑝𝑘⋅𝑙𝑜𝑔(𝑝𝑘),

где 𝑝𝑘pk​ - доля объектов 𝑘k-го класса в вершине,  а 𝐾K - число классов в задаче.

Энтропия обладает тем свойством, которое мы хотим: 𝐻(𝑅)=0H(R)=0 в ситуации, когда одно значение 𝑝𝑖=1pi​=1, а все остальные 0. При этом она принимает наибольшее значение в ситуации, когда классы перемешаны.

- Критерий Джини:

𝐻(𝑅)=∑𝑘=1𝐾𝑝𝑘⋅(1−𝑝𝑘)

где 𝑝𝑘​ - доля объектов k-го класса в вершине,  а 𝐾K - число классов в задаче.

Критерий Джини также отвечает нашим запросам: 𝐻(𝑅)=0H(R)=0 в случае, когда в вершине все объекты одного класса, и 𝐻(𝑅)H(R) максимален в случае максимальной перемешанности объектов.

# **Подбор гиперпараметров**

Вы уже знаете, что деревья переобучаются, поэтому для снижения переобучения у них настраивают гиперпараметры. 

Гиперпараметров у дерева довольно много. Если посмотреть документацию sklearn, то мы увидим довольно много различных гиперпараметров. Основные из них:

- _criterion_ - критерий информативности, используемый при построении дерева
- _max_depth_ - максимальная глубина дерева
- _min_samples_split_ - минимальное количество объектов, которые должны находиться в вершине, чтобы её дальше разбивать
- _min_samples_leaf_ - минимальное количество объектов, которое находится в листе (если после разбиения в одной из полученных подгрупп число объектов меньше, чем min_samples_leaf, то разбиение не производится)
- _max_features_ - число признаков, используемых для поиска наилучшего предиката в каждой вершине.

Посмотрим для примера, как влияют некоторые гиперпараметры на _случайный лес_ (на его сложность и обобщающую способность).

1) max_depth:

Чем больше глубина дерева, тем больше оно подгоняется под данные. Но при совсем маленькой глубине дерево может не уловить нужных зависимостей в данных и недообучиться.
2) min_samples_leaf:

Если ограничения на число объектов в листе нет (min_samples_leaf = 1), то дерево будет строиться до полной подгонки под ответ (если нет других ограничений). Чем больше минимальное число объектов в листе, тем менее дерево переобучено (подгоняется под ответ). При слишком больших значениях гиперпараметра есть риск недообучиться.

**Стрижка дерева**

Существует еще один интересный способ ограничения сложности дерева - стрижка (pruning).
Есть разные алгоритмы стрижки. Мы рассмотрим _Cost-Complexity pruning_ - стрижку, заключающуюся в добавлении регуляризации к функционалу ошибки дерева.

Пусть 𝑄(𝑇) - функционал ошибки, минимизируемый при построении дерева 𝑇T. Добавим регуляризацию:

𝑄𝛼(𝑇)=𝑄(𝑇)+𝛼∣𝑇∣

где ∣𝑇∣ - число вершин в дереве 𝑇, 𝛼- коэффициент регуляризации.

При оптимизации такого функционала в итоговом дереве штрафуется число вершин, то есть построенное дерево будет содержать меньше вершин, чем без регуляризации - значит, будет проще и менее переобученным.

**Обработка пропущенных значений**

Все ранее изученные вами модели не умеют работать с пропусками в данных. У деревьев есть естественное обобщение, которое позволяет обучать деревья и делать предсказания на объектах с пропусками. Разберемся, как это происходит.

Обучение

При обучении дерева всё просто: пусть мы на текущем шаге построения дерева берём предикат 𝑥𝑗<𝑡xj​<t, при этом некоторые объекты выборки имеют пропуски в признаке 𝑥𝑗xj​. Тогда при вычислении функции потерь будем игнорировать эти объекты (обозначим множество объектов с пропуском признака 𝑥𝑗xj​ как 𝑉𝑗Vj​):

$𝑄(𝑅,𝑗,𝑡)≈\frac{∣𝑅∖𝑉𝑗∣}{∣𝑅∣}𝑄(𝑅∖𝑉𝑗,𝑗,𝑡)$

где 𝑅∖𝑉𝑗R∖Vj​ - все объекты в вершине 𝑅R без объектов с пропусками 𝑉𝑗Vj​.

Предсказание

Пусть на этапе предсказания в вершину с предикатом 𝑥𝑗<𝑡 попал объект, у которого пропущено значение признака 𝑥𝑗​. Так как мы не знаем, в какую ветку дерева отправить объект, отправим его и в левую, и в правую ветку. Далее объект по обеим веткам дойдет до листьев и получит какие-то прогнозы. Итоговый прогноз на этом объекте - это среднее значение прогноза с весами, пропорциональными числу объектов в каждой из веток дерева, куда мы отправляем объект:


**Кодирование категориальных признаков**

Деревья в некоторых библиотеках python умеют работать с категориальными (нечисловыми) признаками. Давайте разберемся как именно они это делают.

Во-первых, зачем вообще что-то делать с категориальными признаками? Ведь можно строить решающие деревья и на этих признаках, например:

Такие деревья будут небинарными. Если мы разбиваем вершину по категориальному признаку, то из неё может исходить сразу несколько веток. Получим очень сложные по структуре (а значит, переобученные деревья)

Cтроить небинарные деревья плохо, потому что они будут очень сложными,  а значит, еще более переобученными, чем обычные бинарные деревья.

Можно поступить по-другому: можно строить бинарные деревья, и в предикатах, где используются категориальные признаки, делить множество всех категорий на две группы и исходя из этого отправлять объект в левую или в правую вершину. 
Однако, тут возникает другая проблема: при построении дерева придется перебрать все варианты разбиения категориального признака на две группы - а этих вариантов очень много!

Итак, подведем промежуточный итог, как не стоит делать:

- строить небинарные деревья на исходных категориальных признаках
- строить бинарные деревья, для этого делить категории на две группы (на самом деле, разработчики научились реализовывать этот подход быстро, но сейчас мы не будем на нем останавливаться)

Бинарная классификация (классы 0 и 1)

Посчитаем долю объектов положительного класса для каждого значения категориального признака 𝑥x (мы смотрим только на объекты в некоторой вершине): пусть 𝑝𝑖pi​ - доля объектов положительного класса для 𝑖i-го значения категориального признака, то есть

𝑝𝑖=(число объектов с y = 1 и 𝑥=𝑖) / (число объектов с 𝑥=𝑖).

_**Упорядочим 𝑝𝑖​-е по возрастанию: заменим категорию с наименьшим 𝑝𝑖​ на 1, со следующим 𝑝𝑖​ на 2 и так далее.**_ 

И с такой кодировкой дальше будем разбивать объекты на группы, имея уже числовой признак. 

Почему это работает?

Потому что по сути нам надо знать, как влияет признак (категория) на ответ, и мы для каждой категории вычисляем долю объектов положительного класса, что и измеряет влияние на ответ.

Регрессия

В задаче регрессии будем действовать похожим образом. Для каждой категории посчитаем число 𝑚𝑖mi​ - средний ответ на объектах из данной категории, то есть

𝑚𝑖=mi​= (сумма значений целевой переменной для объектов с 𝑥=𝑖x=i) / (число объектов с 𝑥=𝑖x=i).

_**Упорядочим 𝑚𝑖mi​-е по возрастанию: заменим категорию с наименьшим 𝑚𝑖mi​ на 1, со следующим 𝑚𝑖mi​ на 2 и так далее.**_

[[Practical_ML_content]]