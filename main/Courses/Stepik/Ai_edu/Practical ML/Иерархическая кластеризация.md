
Другой подход к кластеризации - иерархическая кластеризация, основанная на некоторых свойствах графов.

В графовом подходе мы можем представить данные в виде графа, вершинами которого являются объекты, а на ребрах стоят расстояния между объектами.

- Идея метода состоит в том, что изначально у нас есть полный граф - то есть граф, в котором каждая вершина соединена с каждой ребром
- Затем мы выкидываем из графа все ребра, которые соединяют вершины, удаленные друг от друга больше, чем на некоторое выбранное расстояние. Граф распадается на связные компоненты, каждая из которых образует кластер.

Иерархическая (агломеративная) кластеризация - это алгоритм, в котором мы последовательно объединяем маленькие кластеры в более крупные по алгоритму:

- на первом шаге 1 объект = 1 кластер
- на следующем шаге мы объединяем два ближайших объекта в 1 кластер
- на следующем шаге объединяем два ближайших кластера в один и так далее
- на последнем шаге имеем один кластер, содержащий все объекты

 **(стратегии склеивания кластеров)**

Существуют разные подходы к объединению кластеров в один (linkage):

- Average linkage - считаем расстояние между кластерами как среднее расстояние между центрами кластеров
- Maximum (complete) linkage - считаем расстояние между кластерами как расстояние между наиболее удаленными точками из двух кластеров
- Single linkage - считаем расстояние между кластерами как расстояние между двумя наиболее близкими точками из двух кластеров


**выбор числа кластеров)**


Для этого существует очень удобная визуализация результата - _дендрограмма_.
- По оси x на дендрограмме отмечены изначальные объекты
- По оси y - порог расстояния между объектами (а затем между кластерами)

При увеличении значения по оси y некоторые объекты (а затем кластеры) склеиваются друг с другом. На последнем уровне все объекты склеиваются в один кластер.

В sklearn можно выбрать конкретный порог для склейки. Тогда все кластеры, полученные по разбиению этим порогом, будут покрашены в свои цвета. Например, на картинке выбран порог 0.94. По этому порогу мы получаем 5 различных кластеров.

Существует правило определения оптимального числа кластеров (если заранее неизвестно): нужно выбирать такой порог, при небольшом изменении которого вверх или вниз не происходит массовой перестройки кластеров. То есть порог, вокруг которого картина более-менее стабильна.

[[DBSCAN, HDBSCAN]]
[[Кластеризация данных]]