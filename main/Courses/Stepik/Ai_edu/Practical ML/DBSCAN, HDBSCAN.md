У метода есть два гиперпараметра:

- размер окрестности (окружности) 𝜀
- минимальное число точек в окрестности min_samples


Алгоритм следующий:

- выбираем произвольную точку (например, А) и проводим вокруг нее окружность заданного радиуса
- считаем, сколько точек попадает в проведенную окружность:
    - если в окружность попадает ≥≥ точек, чем min_samples, то точку называем _основной (core point)_ и красим в красный, и переходим в любого ее соседа из окрестности, которого мы еще не посетили
    - если в окружности попадает меньше точек, чем min_samples (но больше одной - самой точки), то мы называем точку _граничной (border point)_ и красим ее в желтый. Затем переходим в любую точку, которую еще не посетили
    - если в окружность попала только сама выбранная точка, то мы называем ее _шумовой (noise point)_ и красим в синий. Затем переходим в любую точку, которую еще не посетили
- так обходим все точки

В итоге все точки, где мы можем пройти по пути от соседа к соседу, без перепрыгиваний (то есть группы core + border points) образуют кластеры.



# **Hierarchical DBSCAN**

Несмотря на то, что DBSCAN умеет находить кластеры сложной формы, не все задачи ему по плечу. Проблемы возникают, когда у кластеров разная плотность. Из-за этого невозможно подобрать оптимальные гиперпараметры, при которых DBSCAN верно находит кластеры и не называет какие-то кластеры или их часть шумом.

Шаг 1: В DBSCAN мы ходили от точки к точки с шагом 𝜀 - фиксированным радиусом окружности. Если мы хотим учесть разную плотность точек в кластерах, нам придется обобщить эту процедуру.

- Пусть $𝑐𝑜𝑟𝑒_𝑘(𝑥)$- расстояние от точки 𝑥 до 𝑘-го ближайшего соседа (приближение плотности)
- Тогда расстояние между точками 𝑎 и 𝑏:

𝑑(𝑎,𝑏)=𝑚𝑎𝑥{𝑐𝑜𝑟𝑒𝑘(𝑎),𝑐𝑜𝑟𝑒𝑘(𝑏),𝜌(𝑎,𝑏)},

где 𝜌(𝑎,𝑏)) - расстояние между точками 𝑎 и 𝑏 по исходной метрике (например, евклидово расстояние).

Благодаря такому способу вычисления расстояния:

- в области высокой плотности точек новое расстояние 𝑑(𝑎,𝑏)=𝜌(𝑎,𝑏) равно исходному
- в области низкой плотности новое расстояние 𝑑(𝑎,𝑏) равно какому-то из 𝑐𝑜𝑟𝑒𝑘corek​ расстояний

То есть благодаря метрике 𝑑 мы оставляем плотные точки близкими друг к другу, а далекие точки - еще больше разводим друг относительно друга.

Шаг 2:

После того, как мы научились хитрым образом считать расстояние 𝑑 между объектами, на основе этих расстояний строим минимальное остовное дерево

Шаг 3: на основе полученного дерева можно построить дендрограмму. Для этого сначала упорядочиваем вершины дерева по расстоянию (в порядке возрастания) и проходим по ним, итеративно образуя новые кластеры для каждой вершины.

Затем, чтобы выбрать число кластеров, мы можем провести горизонтальную линию по некоторому выбранному порогу - на самом деле ровно это и делает _DBSCAN_ (можно показать, что алгоритм _DBSCAN_ эквивалентен тому, что мы описали в шагах 1-3).

Шаг 4: Сжимаем дендрограмму.

Алгоритм имеет гиперпараметр _min_cluster_size_. Когда гиперпараметр задан, мы идем по дендрограмме сверху вниз, и в каждый момент, когда один кластер распадается на два, мы проверяем, что в двух новых кластерах число объектов больше, чем min_cluster_size:

- если в каком-то кластере число объектов меньше, чем min_cluster_size, то мы выкидываем точки этого кластера и дальше не рассматриваем
- если в обоих кластерах больше, чем min_cluster_size объектов, мы считаем, что это верное деление на кластеры, и сохраняем оба полученных кластера

В итоге мы получаем сжатую дендрограмму:

Шаг 5: Наконец, осталось выбрать кластеры. Правило следующее:

- если суммарная площадь кластеров больше, чем площадь их основания, то мы оставляем кластеры как есть
- если суммарная площадь кластеров меньше, чем площадь их основания, то мы склеиваем кластеры в один


[[Кластеризация данных]]