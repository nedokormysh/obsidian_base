
# Определение Decision tree

Решающие деревья – это целый класс моделей, они позволяют восстанавливать нелинейные зависимости произвольной сложности. В общем случае мы имеем бинарное дерево, где на каждой вершине есть условие.

В конечном итоге попадаем в лист, в котором записан прогноз, который и выдается в качестве ответа модели.

Определения:
- Вершины - от которых возможно несколько вариантов, называют узлами. Они показывают возможные ситуации (точки принятия решений);
- Конечные узлы (листья) - представляют результат (значение целевой функции);
- Ребра (ветви) - соединяющие узлы, описывают вероятности развития событий по этому сценарию. Можно строить и более сложные не бинарные деревья, но, как правило, используются именно бинарные. Этого достаточно, чтобы решать большинство задач

Наиболее частое условие (предикат) в вершинах простое – мы проверяем, находится ли значение j-того признака левее, чем некоторый порог.
То есть мы берем у объекта j-тый признак, сравниваем с порогом t, и если оно меньше или равно порогу, мы идем влево, если больше порога, мы идем вправо:
[𝑥! ≤ 𝑡]

Виды задач в решающих деревьях:
• Классификация: прогнозом в листе будет либо сам
класс, либо вероятность классов
• Регрессия: вещественное число

В случае, если существует несколько классов с одинаковой и наивысшей вероятностью, классификатор будет предсказывать класс с наименьшим индексом среди этих
классов.

# Построение деревьев

Для любой выборки можно построить решающее дерево, не допускающее на нем ни одной ошибки – где в каждом листе находится ровно по одному объекту.
Подобное дерево с одним объектом в листе будет скорее всего переобученным.

Можно поставить задачу, где необходимо найти минимальное дерево (с точки зрения количества листьев/глубины дерева), которое будет обладать обобщающими способностями, среди всех деревьев, не допускающих ошибок на обучении. 

Жадные алгоритмы заключаются в принятии локально оптимальных решений на каждом этапе, допуская, что конечное решение также окажется оптимальным.

Конкретный метод построения решающего дерева определяется:
1. Видом предикатов (правил) в вершинах
2. Функционалом качества 𝑄 𝑋, 𝑗, 𝑡
3. Критерием останова
4. Методом обработки пропущенных значений
5. Методом стрижки

## Алгоритм (жадный алгоритм)
1. Берется вся обучающая выборка X, находится наилучшее ее разбиение на две части
$𝑅_1(𝑗, 𝑡) = {𝑥|𝑥_j ≤ 𝑡}$ и $𝑅_2(𝑗, 𝑡) = {𝑥|𝑥_j > 𝑡}$ с точки зрения
заранее заданного критерия ошибки 𝑸(𝑿, 𝒋, 𝒕)
2. Отыскав наилучшие значения 𝑗 (индекс объекта) и 𝑡 (порог) при котором
$𝑄(𝑋, 𝑗, 𝑡) → min_{j,t}$
, создается корневая вершину дерева, ей в соответствие ставится разбиение $[𝑥_j ≤ 𝑡]$
3. Далее мы разбиваем нашу вершину на две: левую и правую. При этом часть объектов, а именно те, на которых 𝑗-тый признак меньше или равен порогу 𝑡, отправляются влево 𝑋_l , а часть объектов, у которых значение 𝑗- того признака больше порога 𝑡, отправляются вправо 𝑋_r
4. Для каждой из этих подвыборок процедура повторяется снова(рекурсивно), строятся дочерние вершины, тем самым углубляется наше дерево
5. В каждой вершине мы проверяем выполнение условия останова (критерий останова) — и если выполнилось, то прекращаем разбиение и объявляем эту вершину листом
6. Когда дерево построено, каждому листу ставится в соответствие ответ.

## Предсказание в листе

• В случае с классификацией это может быть класс, к которому относится больше всего объектов в листе, или вектор вероятностей (скажем, вероятность класса может быть равна доле его объектов в листе)
• В случае, если существует несколько классов с одинаковой и наивысшей вероятностью, классификатор будет предсказывать класс с наименьшим индексом среди этих классов.
• Для регрессии это может быть среднее значение, медиана или другая функция от целевых переменных объектов

## Критерий ошибки

, при котором находится наилучшее разбиение, при котором мы ищем
лучшие j и t:
$𝑸(𝑿, 𝒋, 𝒕) =\frac{𝑿𝒍}{𝑿}𝑯(𝑿𝒍) +\frac{𝑿𝒓}{𝑿}𝑯(𝑿𝒓)$
Здесь 𝐻(𝑋)— это критерий информативности (impurity criterion), на вход передаются
выборки 𝑋l и 𝑋r.
𝐻 – измеряет качество подмножества, насколько сильный разброс ответов имеет место при
попадании выборки 𝑋l в левое поддерево и 𝑋r соответственно в правое. Чем меньше
разнообразие целевой переменной, тем меньше должно быть значение критерия ошибки.
Обратите внимание, что значения 𝑿𝒍 и 𝑿𝒓 нормируются на размер выборки 𝑿 (доля
объектов в вершине).

## Критерий информативности для регрессии

Чтобы измерить критерий информативности, сначала вычислим средний ответ выборки - !𝑦 (среднеезначение). А затем вычислим дисперсию выборки, которая вычисляется как среднее значение квадрата отклонения ответа на объекте от среднего ответа по выборке.
Разброс в данном случае — это дисперсия ответов этой выборки.
$\overline{y} (𝑋) =\frac{1}{|X|}\sum_{i∈X}y_i$
$𝐻(𝑋) =\frac{1}{|X|}\sum_{i∈X}(𝑦_i−\overline(𝑦)$
Информативность вершины измеряется её дисперсией — чем ниже разброс целевой переменной, тем лучше вершина.
Также можно использовать и другие функции ошибки 𝐿 — например, при выборе абсолютного отклонения мы получим в качестве критерия среднее абсолютное отклонение от медианы.

## Критерий информативности для классификации
### Джини
Критерий Джини. Обозначим через 𝑝_k долю объектов класса 𝑘 в выборке 𝑋, где 𝑘 ∈ {1, . . . ,𝐾}:
$𝑝_k =\frac{1}{|X|}\sum_{(x_i,y_i)∈X}[𝑦_i= 𝑘]$
$𝐻(𝑋) =\sum_{k=1}^Kp_k(1-p_k)=\sum_{k\neq{k'}}^Kp_kp_{k'}$
Свойства критерия Джинни:
• 𝑝_k – положительное число, так как обозначает долю, значит сам критерий Джини всегда не отрицателен
• Если в выборке X все объекты относятся к одному классу, то первое слагаемое будет равно 1, а второе 0,
то тогда и при перемножении сам критерий Джини будет равен 0. Это означает, что его оптимум
достигается в том случае, если все объекты в выборке относятся к одному классу.
Максимизацию этого критерия можно интерпретировать как максимизацию числа пар объектов одного
класса, оказавшихся в одном поддереве.