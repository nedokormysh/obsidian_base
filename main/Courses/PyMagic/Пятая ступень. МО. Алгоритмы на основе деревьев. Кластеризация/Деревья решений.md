
# Определение Decision tree

Решающие деревья – это целый класс моделей, они позволяют восстанавливать нелинейные зависимости произвольной сложности. В общем случае мы имеем бинарное дерево, где на каждой вершине есть условие.

В конечном итоге попадаем в лист, в котором записан прогноз, который и выдается в качестве ответа модели.

Определения:
- Вершины - от которых возможно несколько вариантов, называют узлами. Они показывают возможные ситуации (точки принятия решений);
- Конечные узлы (листья) - представляют результат (значение целевой функции);
- Ребра (ветви) - соединяющие узлы, описывают вероятности развития событий по этому сценарию. Можно строить и более сложные не бинарные деревья, но, как правило, используются именно бинарные. Этого достаточно, чтобы решать большинство задач

Наиболее частое условие (предикат) в вершинах простое – мы проверяем, находится ли значение j-того признака левее, чем некоторый порог.
То есть мы берем у объекта j-тый признак, сравниваем с порогом t, и если оно меньше или равно порогу, мы идем влево, если больше порога, мы идем вправо:
$[𝑥_j ≤ 𝑡]$

Виды задач в решающих деревьях:
• Классификация: прогнозом в листе будет либо сам класс, либо вероятность классов
• Регрессия: вещественное число

В случае, если существует несколько классов с одинаковой и наивысшей вероятностью, классификатор будет предсказывать класс с наименьшим индексом среди этих
классов.

# Построение деревьев

Для любой выборки можно построить решающее дерево, не допускающее на нем ни одной ошибки – где в каждом листе находится ровно по одному объекту.
Подобное дерево с одним объектом в листе будет скорее всего переобученным.

Можно поставить задачу, где необходимо найти минимальное дерево (с точки зрения количества листьев/глубины дерева), которое будет обладать обобщающими способностями, среди всех деревьев, не допускающих ошибок на обучении. 

Жадные алгоритмы заключаются в принятии локально оптимальных решений на каждом этапе, допуская, что конечное решение также окажется оптимальным.

Конкретный метод построения решающего дерева определяется:
1. Видом предикатов (правил) в вершинах
2. Функционалом качества 𝑄 𝑋, 𝑗, 𝑡
3. Критерием останова
4. Методом обработки пропущенных значений
5. Методом стрижки

## Алгоритм (жадный алгоритм)
1. Берется вся обучающая выборка X, находится наилучшее ее разбиение на две части
$𝑅_1(𝑗, 𝑡) = {𝑥|𝑥_j ≤ 𝑡}$ и $𝑅_2(𝑗, 𝑡) = {𝑥|𝑥_j > 𝑡}$ с точки зрения
заранее заданного критерия ошибки 𝑸(𝑿, 𝒋, 𝒕)
2. Отыскав наилучшие значения 𝑗 (индекс объекта) и 𝑡 (порог) при котором
$𝑄(𝑋, 𝑗, 𝑡) → min_{j,t}$
, создается корневая вершину дерева, ей в соответствие ставится разбиение $[𝑥_j ≤ 𝑡]$
3. Далее мы разбиваем нашу вершину на две: левую и правую. При этом часть объектов, а именно те, на которых 𝑗-тый признак меньше или равен порогу 𝑡, отправляются влево 𝑋_l , а часть объектов, у которых значение 𝑗- того признака больше порога 𝑡, отправляются вправо 𝑋_r
4. Для каждой из этих подвыборок процедура повторяется снова(рекурсивно), строятся дочерние вершины, тем самым углубляется наше дерево
5. В каждой вершине мы проверяем выполнение условия останова (критерий останова) — и если выполнилось, то прекращаем разбиение и объявляем эту вершину листом
6. Когда дерево построено, каждому листу ставится в соответствие ответ.

## Предсказание в листе

• В случае с классификацией это может быть класс, к которому относится больше всего объектов в листе, или вектор вероятностей (скажем, вероятность класса может быть равна доле его объектов в листе)
• В случае, если существует несколько классов с одинаковой и наивысшей вероятностью, классификатор будет предсказывать класс с наименьшим индексом среди этих классов.
• Для регрессии это может быть среднее значение, медиана или другая функция от целевых переменных объектов

## Критерий ошибки

, при котором находится наилучшее разбиение, при котором мы ищем
лучшие j и t:
$𝑸(𝑿, 𝒋, 𝒕) =\frac{𝑿𝒍}{𝑿}𝑯(𝑿𝒍) +\frac{𝑿𝒓}{𝑿}𝑯(𝑿𝒓)$
Здесь 𝐻(𝑋)— это критерий информативности (impurity criterion), на вход передаются
выборки 𝑋l и 𝑋r.
𝐻 – измеряет качество подмножества, насколько сильный разброс ответов имеет место при попадании выборки 𝑋l в левое поддерево и 𝑋r соответственно в правое. Чем меньше разнообразие целевой переменной, тем меньше должно быть значение критерия ошибки.
Обратите внимание, что значения 𝑿𝒍 и 𝑿𝒓 нормируются на размер выборки 𝑿 (доля
объектов в вершине).

## Критерий информативности для регрессии

Чтобы измерить критерий информативности, сначала вычислим средний ответ выборки - !𝑦 (среднеезначение). А затем вычислим дисперсию выборки, которая вычисляется как среднее значение квадрата отклонения ответа на объекте от среднего ответа по выборке.
Разброс в данном случае — это дисперсия ответов этой выборки.
$\overline{y} (𝑋) =\frac{1}{|X|}\sum_{i∈X}y_i$
$𝐻(𝑋) =\frac{1}{|X|}\sum_{i∈X}(𝑦_i−\overline(𝑦)$
Информативность вершины измеряется её дисперсией — чем ниже разброс целевой переменной, тем лучше вершина.
Также можно использовать и другие функции ошибки 𝐿 — например, при выборе абсолютного отклонения мы получим в качестве критерия среднее абсолютное отклонение от медианы.

## Критерий информативности для классификации
### Джини
Критерий Джини. Обозначим через 𝑝_k долю объектов класса 𝑘 в выборке 𝑋, где 𝑘 ∈ {1, . . . ,𝐾}:
$𝑝_k =\frac{1}{|X|}\sum_{(x_i,y_i)∈X}[𝑦_i= 𝑘]$
$𝐻(𝑋) =\sum_{k=1}^Kp_k(1-p_k)=\sum_{k\neq{k'}}^Kp_kp_{k'}$
Свойства критерия Джинни:
• 𝑝_k – положительное число, так как обозначает долю, значит сам критерий Джини всегда не отрицателен
• Если в выборке X все объекты относятся к одному классу, то первое слагаемое будет равно 1, а второе 0, то тогда и при перемножении сам критерий Джини будет равен 0. Это означает, что его оптимум достигается в том случае, если все объекты в выборке относятся к одному классу.
Максимизацию этого критерия можно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве.

## Энтропийный критерий информативности 

Энтропийный критерий аналогично имеет оптимум, который достигается в том случае, если все объекты в подвыборке относятся к одному классу.
$𝐻(𝑋) =-\sum_{k=1}^Kp_kln(p_k)$
Энтропия - это мера случайности или неопределенности. Для этого критерия характерны те же свойства:
• если в выборке X находятся объекты ровно одного класса, например первого, то значение энтропийного критерия будет равно 0
• значение энтропийного критерия не отрицательное
У него так же есть очень интересный физический смысл, этот критерий— это мера отличия распределения классов от вырожденного. Если распределение вырожденное, то энтропия равна 0, в этом распределении нет ничего неожиданного, мы всегда знаем, что мы будем получать из него. Если же это распределение равномерное, то есть вероятность получить каждый класс в этой выборке одинаковая, то энтропия будет максимальна.

## Прирост информации

Поскольку энтропия – это по сути степень хаоса (или неопределенности) в
системе, уменьшение энтропии называют приростом информации.
$𝐼𝐺(𝑄) = 𝐻_0 −\sum_{i=1}^q \frac{N_i}{N}H_i$

• где 𝑞 – число групп после разбиения
• 𝑁_i – число элементов выборки, у которых признак 𝑄 имеет i-ое значение
• 𝑁 – общее число элементов

## Дополнение

• Для обработки количественных признаков в деревьях решений количественный
признак сортируется по возрастанию, и в дереве проверяются только те пороги,
при которых целевой признак меняет значение.
• Когда в данных много количественных признаков, и у каждого много
уникальных значений, могут отбираться не все пороги, описанные выше, а
только топ-N, дающих максимальный прирост все того же критерия.

В дерево решений можно построить до такой глубины, чтоб в каждом листе был ровно один
объект.
Но на практике это не делают (если строится только одно дерево) из-за того, что такое дерево будет переобученным – оно слишком настроится на обучающую выборку и будет плохо делать прогнозы на новых данных.
Есть два исключения, когда деревья строятся до максимальной глубины:
• Случайный лес (композиция многих деревьев) усредняет ответы деревьев, построенных до
максимальной глубины (почему стоит делать именно так, разберемся позже)
• Стрижка дерева (pruning). При таком подходе дерево сначала строится до максимальной
глубины, потом постепенно, снизу вверх, некоторые вершины дерева убираются за счет
сравнения по качеству дерева с данным разбиением и без него (сравнение проводится с
помощью кросс-валидации).

## Критерии останова

• Ограничение глубины дерева
• Ограничение максимального количества листьев в дереве
• Ограничение минимального числа объектов в листе
• Остановка построения дерева в случае, если все объекты в листе относятся к
одному классу
• Ограничение на минимальное количество объектов для разбиения в вершине


## Гиперпараметры Decision Tree:

• max_depth – максимальная глубина дерева (рекомендуется от 4 до 10)
• splitter – стратегия разбиения на каждом листе {“best”, “random”}, default=”best”
• min_samples_leaf – минимальное количество объектов в листе, после которого не нужно делать разбиение $int: [1, inf), float: (0,1]$.
• Не забываем, что при 𝑛 = 1 дерево будет переобучено. Нет каких-то четких правил по выбору числа, но обычно берут 𝑛 = 5.
• min_samples_split – минимальное количество объектов для разбиения в вершине $int: [2, inf), float: (0,1]$
• max_features - максимальное кол-во признаков, которые следует учитывать при поиске наилучшего разбиения, (0,1], также max_features может быть задан {‘auto’, ‘sqrt’, ‘log2’}
• criterion – критерий разбиения в узле, вершине {“gini”, “entropy”}, default=”gini”
Для классификации дополнительно:
• class_weight - учитывание влияния класса (дисбаланс), "balanced", default=None, также можно задать словарь с весами для классов


## Стрижка дерева (pruning)

Стрижка дерева является альтернативой критериям останова.
• При использовании стрижки сначала строится переобученное дерево
• Далее производится оптимизация структуры дерева с целью улучшения обобщающей способности (стрижка листьев по некоторому критерию)
• Стрижка позволяет достичь лучшего качества по сравнению с ранним остановом построения дерева на основе различных критериев (согласно исследованиям)
• Но, это трудоемкая процедура
Стрижка деревьев имеет смысл только при построении одного дерева, так как деревья сами по себе являются слабыми алгоритмами и не представляют большого интереса.
Больший смысл имеют композиции деревьев. Деревья в них должны быть переобучены (в случайных лесах), либо должны иметь очень небольшую глубину (в бустинге), из-за чего необходимость в стрижке отпадает.