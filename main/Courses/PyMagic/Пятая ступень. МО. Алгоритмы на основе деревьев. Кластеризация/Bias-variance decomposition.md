Дано (задача регрессии):
$𝑋 = (𝑥_l , 𝑦_l)_{i=1}^l$ – обучающая выборка размером 𝑙
$𝐿(𝑦, 𝑎) = (𝑦 − 𝑎 (𝑥) )^2$ - квадратичная функция потерь
$𝑄(𝑎)$ – качество (усреднение ошибки)
$𝔼 [𝑧] = \frac{Σ_{i=1}^Nz_i}{N}$ – краткая запись матожидания
Решение:
$𝑄(𝑎) = 𝔼[(𝑦 − 𝑎 𝑥 )^2] = 𝔼[𝑦^2] + 𝔼[𝑎(𝑥)^2] − 2𝔼[𝑦𝑎(𝑥) ] = 𝔼[𝑦^2] + 𝔼[ \hat{𝑦}^2] − 2𝔼[𝑦 \hat{𝑦}]$
В силу линейности математического ожидания справедлива формула для дисперсии по свойству мат ожидания:
$𝐷[𝑋] = 𝑉𝑎𝑟[𝑋] = 𝔼[𝑋 − (𝔼[𝑋])^2$
$𝔼[𝑋^2] = 𝑉𝑎𝑟(𝑋) + (𝔼[𝑋])^2$

Проспустил вывод формулы
$𝑸(𝒂) = 𝔼[𝑦^2] + 𝔼[ \hat{𝑦}^2] − 2𝔼[𝑦\hat{𝑦}]= 𝑩𝒊𝒂𝒔 (\hat{𝒚}^2)+ 𝑽𝒂𝒓 (\hat{𝒚}) + 𝝈^𝟐$

**Смещение (bias)**
Показывает, насколько хорошо с помощью данного метода обучения и семейства алгоритмов можно приблизить оптимальный алгоритм.
Сгенерируем несколько обучающих выборок и обучим модели. Смещением называется отклонение среднего ответа на основании обученных моделей 𝔼[𝑦 ̂ ] от ответа идеального алгоритма 𝑓.

**Дисперсия (variance)**
Показывает, насколько сильно может изменяться ответ обученного алгоритма в зависимости от выборки — иными словами, она характеризует чувствительность метода обучения к изменениям в выборке.
Сгенерируем несколько обучающих выборок и обучим модели. Разброс ответов у данных моделей и будет являться дисперсией.

**Шум**
Неустранимый шум в данных
