
LightGBM (Light Gradient Boosting Machine) – представляет собой бесплатную распределенную структуру повышения градиента с открытым исходным кодом для машинного обучения, первоначально разработанную Microsoft.
Основные моменты:
• Хорошая вычислительная мощность, работает быстрее, чем XGBoost (оптимизирован, разработка велась на C++)
• Требует меньше памяти для запуска и способен обрабатывать большие объемы данных
• Поддержка параллельных вычислений, использование GPU
• Умеет работать с категориальными данными
• Хороший performance (особенно на больших датасетах)
• Расщепляет дерево по листу в отличие от других алгоритмов бустинга, которые растят деревья по уровням
• Оптимизация поиска наилучшего разбиения при помощи гистограммного метода (LightGBM использует алгоритмы на основе гистограмм, которые объединяют непрерывные значения признаков (атрибутов) в дискретные ячейки. Это ускоряет обучение и сокращает использование памяти)
• Хорошо работают с разряженными данными
• Gradient-based One-Side Sampling (отбирает экземпляры на основе градиентов. Экземпляры с небольшими градиентами хорошо обучены (небольшая ошибка обучения), а экземпляры с большими градиентами недостаточно обучены)
НО: LightGBM не предназначен для небольшого объема наборов данных. Может легко переобучиться на небольшом объеме данных из-за своей чувствительности. Его можно использовать для данных, содержащих более 10 000+ строк (эмпирически). Время inference может быть разным, так как деревья несимметричные, поэтому берем среднее время на больших выборках.


Повышение градиента работает путем последовательного добавления предикторов к
ансамблю, каждый из которых корректирует своего предшественника.
LightGBM выращивает деревья по листьям – по вертикали (лучше всего - первым). Он выбирает для роста лист с большими потерями, позволяя вырасти несбалансированному дереву.
При выращивании одного и того же листа он может снизить больше потерь, чем алгоритм, который выращивает дерево по уровням.
Подробнее про то, как происходит обучение на категориальных данных (гистограмный метод с использованием градиентом и гессианов – квадратичная форма функции, описывающая ее поведение во 2ом порядке)

![[Pasted image 20240902152903.png]]

# Подбор гиперпараметров 
для борьбы с переобучением. LightGBM
• lambda_l1 и lambda_l2 определяют регуляризацию L1 или L2, как XGBoost reg_lambda и reg_alpha.
Их сложнее настроить, так как их величина напрямую не коррелирует с переобучением. Однако хорошим диапазоном поиска является (0, 100) для обоих
• min_gain_to_split (аналог gamma в XGBoost). Консервативный диапазон поиска равен (0, 15). Он может быть использован в качестве дополнительной регуляризации в больших сетках параметров
• bagging_fraction принимает значение в пределах (0, 1) и указывает процент train выборок, которые будут использоваться для обучения каждого дерева (аналог subsample в XGBoost), но выбираться они будут случайно без повторного использования. Чтобы использовать этот параметр, вам также необходимо установить bagging_freq в целочисленное значение (ненулевое значение).
• bagging_freq. 0 означает отключить bagging; k означает выполнять bagging на каждой k итерации. В каждой k-ю итерации LightGBM случайным образом выбирает bagging_fracon * 100 % данных для использования в следующих k итерациях
• feature_fraction указывает процент признаков для выборки при обучении каждого дерева (аналог в XGBoost coolsample_by_three). Таким образом, он также принимает значение между (0,1)


[[Courses/PyMagic/Пятая ступень. МО. Алгоритмы на основе деревьев. Кластеризация/Градиентный бустинг|Градиентный бустинг]]