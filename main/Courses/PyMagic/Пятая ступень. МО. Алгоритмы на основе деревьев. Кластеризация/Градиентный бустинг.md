

Случайные леса не всегда применимы, так как нам необходимо строить глубокие переобученные деревья независимо друг от друга, а потом объединять ответ по ним,
это ведет к большим временным затратам и ресурсам, если мы будем ограничивать их снизу, то тогда можем упустить некоторые закономерности.
Поэтому, если у вас многомиллионная выборка и много признаков, то данный алгоритм плохо применим. Чтобы решить сложную задачу, также потребуется большое количество деревьев.
Бустинг позволяет решить вышеизложенные проблемы, так как:
• Последовательно обучает базовые алгоритмы
• Строит алгоритмы на основании предыдущей ошибки (минимизация ошибки)
• Достаточно простых базовых неглубоких алгоритмов


# Бустинг

Бустинг — это процедура последовательного построения композиции алгоритмов машинного обучения, когда каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов.
Бустинг представляет собой жадный алгоритм построения композиции алгоритмов
$𝑎(𝑥) = \sum_{n=1}^N𝑏_n(𝑥)$

# Градиентный бустинг. Общий алгоритм
Дано:
𝑿 – матрица объект-признаки (train)
𝒚 – целевая переменная (train)
𝑙 – кол-во объектов в обучающей выборке
𝑎 – базовый алгоритм
$\frac{1}{l}∑_{i=1}^{l}(𝑦_i − 𝑎(𝑥_i ))^2→ min$
Функционал для минимизации (MSE)

1) 𝑿, 𝒚
Сначала обучим первый базовый алгоритм $𝑏_1(𝑥)$ , который наилучшим образом
приближает целевую переменную.
Построенный алгоритм  $𝑏_1(𝑥)$, скорее всего, работает не идеально.
$$𝒃_1(𝒙) = 𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(𝒚_𝒊 − 𝒂(𝒙_𝒊))^𝟐$$

2) $𝑿, 𝒚 - 𝒃_𝟏(𝒙)$

$$𝒃_2(𝒙) = 𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(s_𝒊^{(1)} − 𝒂(𝒙_𝒊))^𝟐$$

Поэтому, чтобы учесть ошибку предыдущего алгоритма, вычислим, насколько сильно отличаются предсказания $𝑏_1(𝑥)$ от истинных значений 𝑦 (сдвиг): $𝒔_𝒊^{(𝟏)} = 𝒚_𝒊 − 𝒃_𝟏(𝒙_𝒊)$
Обучим на основании сдвига $𝑏_2(𝑥)$. Ожидается, что композиция из двух таких моделей $𝒂_𝟐(𝒙) = 𝒃_𝟏(𝒙) + 𝒃_𝟐(𝒙)$ будет лучше предсказывать целевую переменную 𝒚
3)   $𝑿, 𝒚 - \sum_{n=1}^N𝒃_n(𝒙_i)$


$$𝒃_2(𝒙) = 𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(s_𝒊^{(N)} − 𝒂(𝒙_𝒊))^𝟐$$

Каждый следующих алгоритм будем аналогично строить по остаткам предыдущего

$$s_i^{(N)}=y_i - \sum_{n=1}^{N-1}b_n(x_i)=y_i - a_{N-1}(x_i), i=1,...,l$$
## Градиент

$$L(y,x)=\frac{1}{2}\sum_{i=1}^l(y_i-a(x_i))^2 \rightarrow min$$



Найдем от выражения выше производную от 𝑎_k алгоритма, предположим, что до этого мы уже обучили 𝑘 − 1 алгоритмов


$$\frac{\partial L(y_i,z)}{\partial z}\Bigg|_{z=a_k(x_i)}=\frac{\partial }{\partial z}\frac{1}{2}(y_i-z)^2\Bigg|_{z=a_k(x_i)}=-(y_i-z)\Bigg|_{z=a_k(x_i)}=a_k(x_i)-y_i$$
Вектор сдвигов
$$s_i^{(N)}=-\frac{\partial L(y_i,z)}{\partial z}\Bigg|_{z=a_k(x_i)}=y_i-a_k(x_i)$$
где
$$a_k(x_i)=\sum_{k=1}^Nb_k(x_i)$$
Поэтому каждый следующий алгоритм в бустинге обучается предсказывать антиградиент функции потерь по предсказанию модели
Общая формула для вектора сдвига, без привязки к функции потерь



$$s=-\nabla{F(s)}=\Big(-L'(y_1,b(x_1)),..., -L'(y_l,b(x_l))\Big)$$
## Алгоритм подробно

1) В качестве примера будем минимизировать квадратичную ошибку (обычно берут
дерево решений – базовый алгоритм)
$\frac{1}{l}∑_{i=1}^l(𝑦_i − 𝑎(𝑥_i ))^2→ min_a$

2) Строим первый базовый алгоритм 𝑏_1(𝑥)
$$𝒃_1(𝒙) = 𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(𝒚_𝒊 − 𝒂(𝒙_𝒊))^𝟐$$
3) Теперь мы хотим построить следующий базовый алгоритм 𝒃_𝟐(𝒙_𝒊) , но как учесть
предыдущую ошибку алгоритма и на основании нее обучить новый? Пусть сумма ответов на наших алгоритмах равна истинному значению целевой переменной 
$𝑏_1(𝑥_i) + 𝒃_𝟐(𝒙_𝒊) = 𝑦_i$
Тогда будем обучать второй алгоритм на основании ошибки предыдущего
$𝒃_𝟐(𝒙_𝒊) = 𝑦_i − 𝑏_1(𝑥_i)$
Обозначается как вектор сдвига: $𝑠_i^{(N)} = 𝑦_i − 𝑏_1(𝑥_i)$

$$𝒃_2(𝒙) = 𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(s_i^{1}-a(x_i))^2=𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(𝒚_𝒊 − b_1(𝒙_𝒊)-a(x_i))^𝟐$$


4) Каждый следующих алгоритм будем аналогично строить по остаткам предыдущего 

$$s_i^{(N)}=y_i - \sum_{n=1}^{N-1}b_n(x_i)=y_i - a_{N-1}(x_i), i=1,...,l$$



$$𝒃_n(𝒙) = 𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l(s_i^{(N)}-a(x_i))^2=𝒂𝒓𝒈𝒎𝒊𝒏_{𝒃∈𝑨}\frac{1}{2}\sum_{i=1}^l\Big(\Big(𝒚_𝒊 − \sum_{n=1}^{N-1}b_n(x_i)\Big)-a(x_i)\Big)^𝟐$$

Итог:
Итоговый ответ при обучении алгоритмов на основании ошибок предыдущих


$$a_N(x_i)=\sum_{n=1}^Nb_n(x_i)$$
















[[Пятая ступень. МО. Композиции. Кластеризация._Content]] [[Pymagic_Content]] [[00_ML]]