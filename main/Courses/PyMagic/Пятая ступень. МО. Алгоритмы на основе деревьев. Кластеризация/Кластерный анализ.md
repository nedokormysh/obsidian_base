
Задача относится к классу задач обучения без учителя unsupervised learning – когда у нас отсутствуют метки классов, мы должны по данным выявить некую закономерность, объединить объекты по схожим признакам, чтобы группы (кластеры) были различны друг от друга как можно больше.

Применение:
Разбиение текста по темам (тематическое моделирование), выявление паттернов поведения, либо групп (клиентов по типу поведения), поиск аномалий, построение иерархий объектов.
Постановка задачи
Дано:
𝑋 – пространство объектов
$𝑋_l = \{𝑥_1,…, 𝑥_l\}$ – обучающая выборка
𝜌 – функция расстояния между объектами
Найти:
𝑌 – множество кластеров
𝑎 – алгоритм кластеризации
## Метрики качества кластеризации

### Силуэтный коэффициент (silhouette score)

Используется, когда метки класса изначально неизвестны. Чем выше значение данной метрики, тем больше вероятность, что объекты довольно хорошо сгруппированы в кластеры. Оценивает расстояние как между объектами внутри кластера, так и между объектами соседних кластеров. Значения от -1 до 1, где 1 – идеальное разделение на
кластеры.
Используются такие параметры:
• a – среднее расстояние между объектом и всеми другими объектами в том же кластере
• b – среднее расстояние между объектом и всеми объектами из других кластеров
$𝑠 =\frac{𝑏 − 𝑎}{max(𝑎, 𝑏)}$

График силуэтного коэффициента
По оси OY – размеры кластеров, по оси OX – значение силуэтного коэффициента для каждого объекта, красная пунктирная линия – среднее значение силуэтного
коэффициента по всем кластерам

Интерпретация:
• Значения близкие к 1 указывают на то, что выборка находится далеко от соседних кластеров
• Значение 0 указывает на то, что выборка находится на границе решения между двумя соседними кластерами или очень близок к ней
• Отрицательные значения указывают на то, что эти выборки могли быть назначены неверному кластеру

Рекомендации:
Лучше всего выбирать те случаи с разделением на кол-во
кластеров, когда нет отрицательных значений силуэтного
коэффициента
### Индекс Calinski–Harabasz (Индекс Калински Харабаша)
Индекс представляет собой отношение суммы дисперсии между кластерами и дисперсии внутри кластера для всех  кластеров (где дисперсия определяется как сумма квадратов расстояний), также другими словами индекс основан на расстоянии от точек кластера до их центроидов, а разделимость - на расстоянии от центроид кластеров до глобального центроида.
Оценка высокая, когда кластеры плотные и хорошо разделимы. Индекс Calinski–Harabasz, как правило, больше подходит для выпуклых кластеров, чем для других концепций кластеров, таких как кластеры на основе плотности (например, при помощи DBSCAN).

### Однородность (Homogeneity), полнота и V-мера (использование размеченных данных)
• однородность: каждый кластер содержит только члены одного класса
• полнота: все члены данного класса относятся к одному кластеру
Оба ограничены снизу 0,0 и выше 1,0 (чем выше, тем лучше). Их гармоническое среднее значение,
называемое V-мерой
$𝑣 =\frac{(1 + 𝛽) * ℎ𝑜𝑚𝑜𝑔𝑒𝑛𝑒𝑖𝑡𝑦 * 𝑐𝑜𝑚𝑝𝑙𝑒𝑡𝑒𝑛𝑒𝑠𝑠}{𝛽 * ℎ𝑜𝑚𝑜𝑔𝑒𝑛𝑒𝑖𝑡𝑦 + 𝑐𝑜𝑚𝑝𝑙𝑒𝑡𝑒𝑛𝑒𝑠𝑠}$
𝛽 = 1 по умолчанию, значение меньше 1 - больший вес будет приписан однородности, иначе полноте

# Метод локтя

Метод локтя (Elbow Rule) – один из самых известных методов, с помощью которого вы можете выбрать правильное значение кол-ва кластеров и повысить производительность модели.
Этот эмпирический метод, где вы можете использовать например, сумму квадратов расстояний между точками (K-means), либо учитывать кол-во объектов в кластере.
При k=1 (кол-во кластеров), наш параметр, на который ориентируемся, будет довольно большой. По мере увеличения значения k параметр будет уменьшаться.
Чтобы вычислить кол-во кластеров, строится график между значениями k и параметром. В какой-то момент значение по оси x перестает резко уменьшаться. Эта точка будет считаться оптимальным значением k.
Когда нет определенной точки Если график с величиной по OY довольно плавный, стоит рассмотреть также зависимость силуэтного коэф и других метрик от кол-ва
кластеров, далее применить метод локтя.



