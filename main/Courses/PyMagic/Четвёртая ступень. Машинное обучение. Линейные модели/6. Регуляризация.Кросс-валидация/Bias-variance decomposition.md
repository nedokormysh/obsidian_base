Ошибка на новых тестовых данных раскладывается:
$𝑸(𝒂) = 𝑩𝒊𝒂𝒔 (\hat{𝒚})^ 𝟐 + 𝑽𝒂𝒓(\hat{𝒚}) + 𝝈^𝟐$

**Смещение Bias**
Сгенерируем несколько обучающих выборок и обучим модели. Смещением называется отклонение среднего ответа на основании обученных моделей $𝔼 [\hat{𝑦}]$ от ответа идеального алгоритма 𝑓.
Смещение показывает, насколько хорошо с помощью данного алгоритма можно приблизить истинную зависимость.

$𝑩𝒊𝒂𝒔 (\hat{𝒚})^ 𝟐 = (𝑓 − 𝔼 [\hat{𝑦}])^2$
где $𝔼 [𝑎(𝑥)] = 𝔼[\hat{𝑦}]$ – матожидание

**Разброс Var (дисперсия)**
Сгенерируем несколько обучающих выборок и обучим модели. Разброс ответов у данных моделей и будет являться дисперсией. Разброс характеризует чувствительность алгоритма к изменениям в обучающей выборке.

$𝑉𝑎𝑟(𝑋) = 𝔼[𝑋^2] − (𝔼[𝑋])^2$
где $𝑉𝑎𝑟(𝑋) = \frac{\sum_{i=1}^N(X_i - X)^2}{N}=𝔼[X-𝔼X]^2=𝔼[𝑋^2] − (𝔼[𝑋])^2$

**Шум**
Неустранимый шум в данных
* При высоком смещении наши предсказания хорошо концентрируются в одном месте, но так и не попадают в цель.
 * При высоком разбросе, какая-то часть ответов близка к верным, но вариация ответов довольно большая.

[[Регуляризация. Кросс-валидация]]