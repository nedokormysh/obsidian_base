* **Overfitting** – переобучение, происходит в результате чрезмерной подгонки параметров модели к зависимостям, содержащимся в обучающем множестве.
Ошибка на тестовой выборке значительно больше, чем на тренировочной.
* **Underfitting** – недообучение, если наша модель наоборот, очень простая и не до конца хорошо описывает данные. Часто это про то, что ошибка достаточно большая на тренировочных данных.

[[Bias-variance decomposition]]
# Борьба с переобучением

Признак переобучения большими значениями коэффициентов. Добавить ограничение на коэффициенты модели – штрафы.

- Использование регуляризации
- Выбор наиболее подходящей модели. Например, если у нас есть линейная зависимость
между признаками и целевой переменной
- Увеличение кол-ва набора данных
- Подбор гиперапараметров модели
- Использование ранней остановки (в бустингах)
- Обрезка алгоритмов на основании деревьев
- Проверка исходного датасета (лики в данных, например, не удалили ID)

[[Регуляризация]]

[[Четвёртая ступень. МO. Лин мод._Content]] [[Pymagic_Content]]  [[00_ML]] 