# Лекция 4. (Линейная регрессия)

**Парная регрессия**

- Простейший случай: один признак
- Модель $a(x) = w_1x+w_0$
- Два параметра $w_1$  и $w_0$
- $w_1$ - тангенс угла наклона
- $w_0$ - где прямая пересекает ось ординат

Два признака

- Чуть более сложный случай: два признака
- Модель $a(x) = w_0 + w_1x_1+w_2x_2$
- Три параметра

Много признаков

- Общий случай d признаков
- Модель $a(x) = w_0 + w_1x_1+...+w_dx_d$
- Количество параметров: d+1
- $w_0$ - свободный коэффициент, сдвиг, биас

чем больше w0, тем выше будет прогноз, если все признаки нулевые.

- $w_1, ..., w_d$ - веса/ коэффициенты

Запишем через скалярное произведение:

$a(x) = w_0 + w_1x_1+...+w_dx_d = w_0 + <w,x>$

Будем считать, что есть признак, всегда равный единице:

$a(x) = w_1x_1+...+w_dx_d =w_1*1+...+w_dx_d = <w,x>$

w1 будет выполнять роль свободного коэффициента, потому что он не умножается ни на что, он сам по себе

если мы записываем уравнение, скажем, w умножить на x скалярно равно нулю, то из геометрии известно, что это гиперплоскость, то есть некоторое такое обобщение плоскости на многомерное пространство. W - это вектор нормали, то есть вектор, который перпендикулярен этой плоскости или гиперплоскости в общем случае. Ну, естественно, эта плоскость это все точки, все векторы, которые перпендикулярны нашему вектору нормали, нашему w.

наша модель это же не уравнение wx=0, это просто скалярное произведение w на x, и это означает, что мы просто берём наши объекты и как бы проецируем их на плоскость, то есть мы берем объект, он лежит где-то вот в нашем признаковом пространстве, и мы смотрим, как он проецируется на нашу модель, на нашу гиперплоскость и после этого получаем прогноз.

- W - вектор нормали
- X - вектор объекта
- <w,x> - проекция вектора X на нормаль

**Применимость линейной регрессии:**

- Нет гарантий, что целевая переменная именно так зависит от
признаков
- Надо формировать признаки так, чтобы модель подходила

Пример: предсказание стоимости квартиры:

$a(x) = w_0 + w_1*(площадь) + w_2 * (район) + w_3 * (расстояние до метро)$

площадь - логичный признак

район - категориальный. Поэтому мы не можем просто умножить наш вес на название - следовательно надо обработать.

- Значения признака «район»: $U = {u_1, ..., u_m}$
- Введём новые признаки вместо $x_j:[x_j = u_1], ..., [x_j = u_m]$
- One-hot кодирование

расстояние до метро - имеет нелинейную зависимость, т.е. также нельзя просто так включить в модель

возьмем, например, минимальное и максимальное значение расстояния до метро и этот интервал побьем на отрезки. Края этих отрезков будем обозначать как t0,t1,t2,t3 и так далее, всего, допустим, у нас будет n этих границ, по которым мы побили наш признак на такие корзинки, на такие отрезки. И объявим индикаторы попадания в каждый отрезок новыми признаками, то есть если у нас, например, n отрезков у нас будет n новых признаков первый признак индикатор попадания в первый отрезок, второй признак индикатор попадания во второй отрезок, ну, и так далее и так далее. Каждый признак мы умножим на свой коэффициент w3 и так далее, и в итоге получится, что у нас, например, первый признак в этой серии - это добавка или вычет, может быть, наоборот, если вес отрицательный, к стоимости в случае, если расстояние до метро попало в интервал от 0 до t1.

$a(x) = w_0 + w_1*(площадь) + w_2 * (район) + w_3 * [t_0=<x_3<t_1] + ... + w_{3+n} * [t_{n-1}=<x_3<t_n]$

Линейные модели
• Модель линейной регрессии хороша, если признаки сделаны
специально под неё
• Пример: one-hot кодирование категориальных признаков или
бинаризация числовых признаков

**Матричная форма записи.**

Среднеквадратичная ошибка и задача обучения для линейной регрессии:

$\frac{1}{l}\sum\limits_{i=1}^l(<w, x_i>-y_i)^2\rightarrow \underset{w}{min}$

берём i-ый объект, считаем скалярное произведение

- Матрица — таблица с числами (для простоты)
- Матрица «объекты-признаки»:

$\begin{matrix}
X = \left(
\begin{array}{cccc}
x_{11} & x_{12} & \ldots & x_{1n}\\
x_{21} & x_{22} & \ldots & x_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \ldots & x_{nn}
\end{array}
\right)
\end{matrix} \in ℝ^{ℓ×d}$

Векторы

- Вектор размера d - тоже матрица
- Вектор-строка: $w = (w_1, ..., w_d) \in ℝ^{1×d}$
- Вектор-столбец: $w = (w_1, ..., w_d) \in ℝ^{d×1}$

Матричное умножение

- Только для матриц $A \in ℝ^{m×k}$ и $B \in ℝ^{k×n}$
- Результат $AB = C \in ℝ^{m×n}$
- Правило:

$c_{ij}=\sum\limits_{p=1}^{k}a_{ip}b_{pj}$

Применение линейной модели

$a(x) = w_1x_1+...+w_dx_d =w_1*1+...+w_dx_d = <w,x>$

$\begin{matrix}
\left(
\begin{array}{cccc}
x_{11} & x_{12} & \ldots & x_{1n}\\
x_{21} & x_{22} & \ldots & x_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \ldots & x_{nn}
\end{array}
\right)
\end{matrix}$

$\begin{matrix}
\left(
\begin{array}{cccc}
w_{1} \\
w_{2}\\
\vdots\\
w_{n}
\end{array}
\right)
\end{matrix}$

Хотим получить:

$\begin{matrix}
\left(
\begin{array}{cccc}
\sum\limits_{i=1}^d w_ix_{1i}\\
\sum\limits_{i=1}^d w_ix_{2i}\\
\vdots\\
\sum\limits_{i=1}^d w_ix_{li}
\end{array}
\right)
\end{matrix}$

Это как раз произведение матрицы X на матрицу w

$Xw= \begin{matrix}
\left(
\begin{array}{cccc}
<w, x_1>\\
<w, x_2>\\
\vdots\\
<w, x_l>
\end{array}
\right)
\end{matrix}$

Научились применять, но нам также нужно вычисление ошибки

- Отклонения прогнозов от ответов:

$Xw-y = \begin{matrix}
\left(
\begin{array}{cccc}
<w, x_1> - y_1\\
<w, x_2> - y_2\\
\vdots\\
<w, x_l> - y_l
\end{array}
\right)
\end{matrix}$

- Нужно подсчитать СКО для этого Евклидова норма (обобщение длины вектора на многомерный случай):

$||z||= \sqrt{\sum\limits_{j=1}^n z_j^2}$

$||z||^2= {\sum\limits_{j=1}^n z_j^2}$

- СКО

$\frac{1}{l}||Xw-y||^2=\frac{1}{l}\sum\limits_{i=1}^l(<w, x_i> - y_i)^2$

- mse in numpy

$np.square(X.dot(w)-y).mean()$

**Обучение линейной регрессии**

- MSE для линейной регрессии:

$Q(w_1, ..., w_d)=\frac{1}{l}\sum\limits_{i=1}^l(w_1x_1 +... +w_dx_d - y_i)^2$

Производная

$\lim\limits_{x\to 0} \frac{f(x) - f(x_0)}{x-x_0}=f'(x_0)$

если точка $x_0$ - экстремум и в ней существует производная, то $f'(x_0)=0$

Градиент — вектор частных производных

$\nabla f(x) = (\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_d})$

- Зафиксируем точку $x_0$
- В какую сторону функция быстрее всего растёт?
- В направлении градиента!
- Если градиент равен нулю, то это экстремум

Условие экстремума

- если точка $x_0$ - экстремум и в ней существует производная, то $\nabla f(x_0)=0$
- если функция выпуклая, то экстремум один
- MSE для линейной регрессии — выпуклая! (при некоторых условиях)

Как раз таки среднеквадратичная ошибка для линейной регрессии - это выпуклая функция, то есть вообще все замечательно, у нас всего один экстремум, считаем градиент, приравниваем нулю, находим решение, и это будет решение нашей задачи. Это можно сделать: давайте возьмем сразу в матричном виде нашу среднеквадратичную ошибку для линейной

$\nabla \frac{1}{l}||Xw-y||^2 = \frac{2}{l}X^T(Xw - y)$

Приравниваем нулю и решаем систему линейных уравнений:

$w=(X^TX)^{-1}X^Ty$

Т.е. можно через градиент получить аналитическое решение (формулу) для линейной регрессии на среднеквадратичной ошибке.

Проблемы аналитического решения.

- если ковариационная матрица вырожденная, то обратной к ней не существует
- Даже если она почти вырожденная, всё равно будут проблемы: численные проблемы найти обратную матрицу
- Если признаков много, то придётся долго ждать(если матрица имеет размер n

на n, то найти обратную к ней, это примерно n в кубе операции, такого порядка сложность)

- что если функция потерь была бы другая, скажем, модуль отклонения или какая-нибудь вообще более хитрая, скажем, там проценты какие-нибудь считали, относительные отклонения - не факт что получилось бы в аналитическом виде найти решение, то есть вот тоже не универсальный подход совсем.

**Проблемы переобучения.**

Нелинейная задача.

Симптомы переобучения:

- Большие коэффициенты отличные от нуля.
- Это эмпирическое наблюдение.
- малейшие изменения признака в такой модели приводит к огромному изменению прогноза, но, очевидно, что эта модель не рабочая, она не научилась обобщать, она вот, наверное, для каких-то ****иксов из обучающей выборке выдает правильный ответ, но на новых данных будет все очень плохо

**Регуляризация.**

Ридж:

Запретим нашей модели иметь большие веса. Будем штрафовать её за такое. Возьмём ошибку и добавим регуляризатор

- Пример фунцкционала: $Q(a,X)=\frac{1}{l}\sum\limits_{i=1}^l(<w, x_i> - y_i)^2$
- Регуляризатор $||w||^2=\sum\limits_{j=1}^d w_j^2$

Получаем новый регуляризованный функционал

$\frac{1}{l}\sum\limits_{i=1}^l(<w, x_i> - y_i)^2+\lambda||w||^2 \rightarrow \underset{w}{min}$

Лямбда - коэффициент регуляризации. Лямбда - это коэффициент регуляризации, который говорит о том, насколько нам важно чтобы веса были небольшими, если лямбду ****поставить, например, огромной, то регуляризация будет сильной.

Можно показать, что в этой задаче у нас все еще функционал выпуклый, то есть там решение будет одно, можно посчитать градиент и можно найти решение этой задачи. 

$w=(X^TX+\lambda I)^{-1}X^Ty$

такая задача, где мы добавляем с некоторым коэффициентом лямбда функционал норму весов, квадрат нормы весов называется гребневой регуляризацией. Ридж

Коэффициент регуляризации с точки зрения обучающей выборки оптимально брать

нулевым.Ну, логично, на обучающей выборке мы хотим уменьшать ошибку, а регуляризатор мешает подгоняться под обучающую выборку, поэтому нужно подбирать коэффициент регуляризации, этот гиперпараметр, по отложенной выборке или по кросс-валидации

в регуляризатор нельзя включать свободный коэффициент, w0, обычно регуляризуют только те коэффициенты, которые умножаются на признаки

Если целевая переменная какая-нибудь большая, например, целевая переменная будет иметь масштаб миллионов. Если вы потребуете, чтобы вообще все веса были близки к нулю, то вряд ли ваша модель сможет выдавать миллионы.

Лассо

- $\frac{1}{l}\sum\limits_{i=1}^l(<w, x_i> - y_i)^2+\lambda\sum\limits_{j=1}^d|w_j| \rightarrow \underset{w}{min}$
- некоторые веса зануляются. Чем больше лямбда, тем больше весов зануляются
- приводит к отбору признаков

Названия

$||z||_2=\sqrt {\sum\limits_{j=1}^d z_j^2}$ - L2 норма

$||z||_1=\sum\limits_{j=1}^d |z_j|$ - L1 норма

Признаки следует масштабировать.

- Вычисляем среднее и стандартное отклонение признака на
обучающей выборке:

$\mu_j=\frac{1}{l}\sum\limits_{i=1}^{l} x_i^j$

$\sigma = \sqrt{\frac{1}{l}\sum \limits_{i=1}^l(x_i^j - \mu_j)^2}$

- Вычтем из каждого значения признака среднее и поделим на
стандартное отклонение:

$x_i^j=\frac{x_i^j - \mu_j}{\sigma_j}$

После этого можем сравнивать признаки между собой.

Другие способы масштабирования: можно масштабировать на 0.1: вычесть минимальное значение признака и поделить на разницу между максимальным и минимальным

Если модель переобучается, то она не имеет смысла, веса по сути используются, чтобы запоминать обучающую выборку, а не для того, чтобы моделировать истинные зависимости между признаками и целевой переменной, поэтому важно не только масштабировать, но и регулиризовать модель перед тем как интерпретировать веса.

Если отмасштабировали и интерпретировали, то тогда веса скорее всего будут интерпретируемы.

Ещё одна вещь: лучший способ - это выбрасывание признаков и измерения того, насколько ошибка увеличилась, но это долго, но, если признаков много, нужно каждый выкинуть, обучить заново модель посчитать качество, может не хватить времени, поэтому в принципе смотреть на абсолютные значения весов при масштабировании и регуляризации не такой плохой вариант.

Пример: у нас два признака. И второй более важен. Но при этом он там принимает значения 1 и 0. Но в большинстве случаев 0. Тогда вроде бы вес у него больше, но он редко бывает не равен нулю. И получается, что его выбрасывание приведёт к меньшему росту ошибки, чем если выбросить первый признак.